const Redis = require('ioredis')
const config = require('../../config/config')
const logger = require('../utils/logger')
const postgresStore = require('./postgresStore')
const goRedisProxy = require('./goRedisProxy')

// æ—¶åŒºè¾…åŠ©å‡½æ•°
// æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°çš„ç›®çš„æ˜¯è·å–æŸä¸ªæ—¶é—´ç‚¹åœ¨ç›®æ ‡æ—¶åŒºçš„"æœ¬åœ°"è¡¨ç¤º
// ä¾‹å¦‚ï¼šUTCæ—¶é—´ 2025-07-30 01:00:00 åœ¨ UTC+8 æ—¶åŒºè¡¨ç¤ºä¸º 2025-07-30 09:00:00
function getDateInTimezone(date = new Date()) {
  const offset = config.system.timezoneOffset || 8 // é»˜è®¤UTC+8

  // æ–¹æ³•ï¼šåˆ›å»ºä¸€ä¸ªåç§»åçš„Dateå¯¹è±¡ï¼Œä½¿å…¶getUTCXXXæ–¹æ³•è¿”å›ç›®æ ‡æ—¶åŒºçš„å€¼
  // è¿™æ ·æˆ‘ä»¬å¯ä»¥ç”¨getUTCFullYear()ç­‰æ–¹æ³•è·å–ç›®æ ‡æ—¶åŒºçš„å¹´æœˆæ—¥æ—¶åˆ†ç§’
  const offsetMs = offset * 3600000 // æ—¶åŒºåç§»çš„æ¯«ç§’æ•°
  const adjustedTime = new Date(date.getTime() + offsetMs)

  return adjustedTime
}

// è·å–é…ç½®æ—¶åŒºçš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
function getDateStringInTimezone(date = new Date()) {
  const tzDate = getDateInTimezone(date)
  // ä½¿ç”¨UTCæ–¹æ³•è·å–åç§»åçš„æ—¥æœŸéƒ¨åˆ†
  return `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(2, '0')}-${String(
    tzDate.getUTCDate()
  ).padStart(2, '0')}`
}

// è·å–é…ç½®æ—¶åŒºçš„å°æ—¶ (0-23)
function getHourInTimezone(date = new Date()) {
  const tzDate = getDateInTimezone(date)
  return tzDate.getUTCHours()
}

// è·å–é…ç½®æ—¶åŒºçš„ ISO å‘¨ï¼ˆYYYY-Wxx æ ¼å¼ï¼Œå‘¨ä¸€åˆ°å‘¨æ—¥ï¼‰
function getWeekStringInTimezone(date = new Date()) {
  const tzDate = getDateInTimezone(date)

  // è·å–å¹´ä»½
  const year = tzDate.getUTCFullYear()

  // è®¡ç®— ISO å‘¨æ•°ï¼ˆå‘¨ä¸€ä¸ºç¬¬ä¸€å¤©ï¼‰
  const dateObj = new Date(tzDate)
  const dayOfWeek = dateObj.getUTCDay() || 7 // å°†å‘¨æ—¥(0)è½¬æ¢ä¸º7
  const firstThursday = new Date(dateObj)
  firstThursday.setUTCDate(dateObj.getUTCDate() + 4 - dayOfWeek) // æ‰¾åˆ°è¿™å‘¨çš„å‘¨å››

  const yearStart = new Date(firstThursday.getUTCFullYear(), 0, 1)
  const weekNumber = Math.ceil(((firstThursday - yearStart) / 86400000 + 1) / 7)

  return `${year}-W${String(weekNumber).padStart(2, '0')}`
}

// å¹¶å‘é˜Ÿåˆ—ç›¸å…³å¸¸é‡
const QUEUE_STATS_TTL_SECONDS = 86400 * 7 // ç»Ÿè®¡è®¡æ•°ä¿ç•™ 7 å¤©
const WAIT_TIME_TTL_SECONDS = 86400 // ç­‰å¾…æ—¶é—´æ ·æœ¬ä¿ç•™ 1 å¤©ï¼ˆæ»šåŠ¨çª—å£ï¼Œæ— éœ€é•¿æœŸä¿ç•™ï¼‰
// ç­‰å¾…æ—¶é—´æ ·æœ¬æ•°é…ç½®ï¼ˆæé«˜ç»Ÿè®¡ç½®ä¿¡åº¦ï¼‰
// - æ¯ API Key ä» 100 æé«˜åˆ° 500ï¼šæä¾›æ›´ç¨³å®šçš„ P99 ä¼°è®¡
// - å…¨å±€ä» 500 æé«˜åˆ° 2000ï¼šæ”¯æŒæ›´é«˜ç²¾åº¦çš„ P99.9 åˆ†æ
// - å†…å­˜å¼€é”€çº¦ 12-20KBï¼ˆRedis quicklist æ¯å…ƒç´  1-10 å­—èŠ‚ï¼‰ï¼Œå¯æ¥å—
// è¯¦è§ design.md Decision 5: ç­‰å¾…æ—¶é—´ç»Ÿè®¡æ ·æœ¬æ•°
const WAIT_TIME_SAMPLES_PER_KEY = 500 // æ¯ä¸ª API Key ä¿ç•™çš„ç­‰å¾…æ—¶é—´æ ·æœ¬æ•°
const WAIT_TIME_SAMPLES_GLOBAL = 2000 // å…¨å±€ä¿ç•™çš„ç­‰å¾…æ—¶é—´æ ·æœ¬æ•°
const QUEUE_TTL_BUFFER_SECONDS = 30 // æ’é˜Ÿè®¡æ•°å™¨TTLç¼“å†²æ—¶é—´

class RedisClient {
  constructor() {
    this.client = null
    this.isConnected = false
    // â±ï¸ ç®¡ç†åå°é¢‘ç¹æŸ¥è¯¢çš„è½»é‡ç¼“å­˜ï¼ˆé¿å…è·¨æœº Redis RTT æ”¾å¤§ï¼‰
    this._apiKeyBindingCountsCache = {
      excludeDeleted: null,
      includeDeleted: null
    }
  }

  async connect() {
    try {
      this.client = new Redis({
        host: config.redis.host,
        port: config.redis.port,
        password: config.redis.password,
        db: config.redis.db,
        retryDelayOnFailover: config.redis.retryDelayOnFailover,
        maxRetriesPerRequest: config.redis.maxRetriesPerRequest,
        lazyConnect: config.redis.lazyConnect,
        tls: config.redis.enableTLS ? {} : false
      })

      this.client.on('connect', () => {
        this.isConnected = true
        logger.info('ğŸ”— Redis connected successfully')
      })

      this.client.on('error', (err) => {
        this.isConnected = false
        logger.error('âŒ Redis connection error:', err)
      })

      this.client.on('close', () => {
        this.isConnected = false
        logger.warn('âš ï¸  Redis connection closed')
      })

      // åªæœ‰åœ¨ lazyConnect æ¨¡å¼ä¸‹æ‰éœ€è¦æ‰‹åŠ¨è°ƒç”¨ connect()
      // å¦‚æœ Redis å·²ç»è¿æ¥æˆ–æ­£åœ¨è¿æ¥ä¸­ï¼Œåˆ™è·³è¿‡
      if (
        this.client.status !== 'connecting' &&
        this.client.status !== 'connect' &&
        this.client.status !== 'ready'
      ) {
        await this.client.connect()
      } else {
        // ç­‰å¾… ready çŠ¶æ€
        await new Promise((resolve, reject) => {
          if (this.client.status === 'ready') {
            resolve()
          } else {
            this.client.once('ready', resolve)
            this.client.once('error', reject)
          }
        })
      }
      return this.client
    } catch (error) {
      logger.error('ğŸ’¥ Failed to connect to Redis:', error)
      throw error
    }
  }

  async disconnect() {
    if (this.client) {
      await this.client.quit()
      this.isConnected = false
      logger.info('ğŸ‘‹ Redis disconnected')
    }
  }

  getClient() {
    if (!this.client || !this.isConnected) {
      logger.warn('âš ï¸ Redis client is not connected')
      return null
    }
    return this.client
  }

  // å®‰å…¨è·å–å®¢æˆ·ç«¯ï¼ˆç”¨äºå…³é”®æ“ä½œï¼‰
  getClientSafe() {
    if (!this.client || !this.isConnected) {
      throw new Error('Redis client is not connected')
    }
    return this.client
  }

  /**
   * ä½¿ç”¨ SCAN è·å–åŒ¹é… pattern çš„æ‰€æœ‰ keyï¼ˆé¿å… KEYS é˜»å¡ï¼‰
   * @param {string} pattern
   * @param {number} count
   * @returns {Promise<string[]>}
   */
  async scanKeys(pattern, count = 20000) {
    const keys = []
    let cursor = '0'

    do {
      const [nextCursor, batch] = await this.client.scan(cursor, 'MATCH', pattern, 'COUNT', count)
      cursor = nextCursor
      if (Array.isArray(batch) && batch.length > 0) {
        keys.push(...batch)
      }
    } while (cursor !== '0')

    return keys
  }

  /**
   * ä½¿ç”¨ SCAN ç»Ÿè®¡åŒ¹é… pattern çš„ key æ•°é‡ï¼ˆé¿å… KEYS é˜»å¡ï¼‰
   * @param {string} pattern
   * @param {(key: string) => boolean} [filter]
   * @param {number} count
   * @returns {Promise<number>}
   */
  async countKeysByScan(pattern, filter = null, count = 20000) {
    let total = 0
    let cursor = '0'

    do {
      const [nextCursor, batch] = await this.client.scan(cursor, 'MATCH', pattern, 'COUNT', count)
      cursor = nextCursor
      if (!Array.isArray(batch) || batch.length === 0) {
        continue
      }

      if (typeof filter === 'function') {
        for (const key of batch) {
          if (filter(key)) {
            total++
          }
        }
      } else {
        total += batch.length
      }
    } while (cursor !== '0')

    return total
  }

  // ğŸ”‘ API Key ç›¸å…³æ“ä½œ
  async setApiKey(keyId, keyData, hashedKey = null) {
    const key = `apikey:${keyId}`
    const client = this.getClientSafe()

    // ç»´æŠ¤å“ˆå¸Œæ˜ å°„è¡¨ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
    // hashedKeyå‚æ•°æ˜¯å®é™…çš„å“ˆå¸Œå€¼ï¼Œç”¨äºå»ºç«‹æ˜ å°„
    const resolvedHashedKey = hashedKey || keyData?.apiKey
    if (resolvedHashedKey) {
      await client.hset('apikey:hash_map', resolvedHashedKey, keyId)
    }

    await client.hset(key, keyData)
    await client.expire(key, 86400 * 365) // 1å¹´è¿‡æœŸ

    // âœ… åŒå†™åˆ° PostgreSQLï¼ˆå¤±è´¥è‡ªåŠ¨å›é€€ï¼Œä¸å½±å“ä¸»æµç¨‹ï¼‰
    if (config.postgres?.enabled && resolvedHashedKey) {
      try {
        await postgresStore.upsertApiKey(keyId, resolvedHashedKey, { id: keyId, ...keyData })
      } catch (error) {
        logger.warn(`âš ï¸ Failed to upsert API key into PostgreSQL: ${error.message}`)
      }
    }
  }

  async updateApiKeyFields(keyId, updates) {
    if (!keyId || !updates || typeof updates !== 'object') {
      return false
    }

    const key = `apikey:${keyId}`
    const client = this.getClientSafe()

    await client.hset(key, updates)
    await client.expire(key, 86400 * 365) // 1å¹´è¿‡æœŸï¼ˆå»¶é•¿æ´»è·ƒKey TTLï¼‰

    if (config.postgres?.enabled) {
      try {
        await postgresStore.patchApiKeyById(keyId, updates)
      } catch (error) {
        logger.warn(`âš ï¸ Failed to patch API key into PostgreSQL: ${error.message}`)
      }
    }

    return true
  }

  async getApiKey(keyId) {
    const client = this.getClientSafe()
    const key = `apikey:${keyId}`

    // âœ… è¯»è·¯å¾„ï¼ˆé˜¶æ®µ3ï¼‰ï¼šä¼˜å…ˆ PostgreSQLï¼ˆè·¨èŠ‚ç‚¹ä¸€è‡´æ€§ï¼‰â†’ miss å†å›é€€ Redisï¼ˆå…¼å®¹è¿ç§»æœŸ/PGå¼‚å¸¸ï¼‰
    if (config.postgres?.enabled) {
      try {
        const pgData = await postgresStore.getApiKeyById(keyId)
        if (pgData) {
          return pgData
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to read API key from PostgreSQL: ${error.message}`)
      }
    }

    const data = await client.hgetall(key)
    if (data && Object.keys(data).length > 0) {
      return data
    }

    // å…¼å®¹å†å²å‰ç¼€
    const legacy = await client.hgetall(`api_key:${keyId}`)
    if (legacy && Object.keys(legacy).length > 0) {
      return legacy
    }

    return null
  }

  async deleteApiKey(keyId) {
    const client = this.getClientSafe()
    const key = `apikey:${keyId}`
    const legacyKey = `api_key:${keyId}`

    // è·å–è¦åˆ é™¤çš„API Keyå“ˆå¸Œå€¼ï¼Œä»¥ä¾¿ä»æ˜ å°„è¡¨ä¸­ç§»é™¤
    let keyData = await client.hgetall(key)
    if (!keyData || Object.keys(keyData).length === 0) {
      keyData = await client.hgetall(legacyKey)
    }

    if (keyData && keyData.apiKey) {
      // keyData.apiKey ç°åœ¨å­˜å‚¨çš„æ˜¯å“ˆå¸Œå€¼ï¼Œç›´æ¥ä»æ˜ å°„è¡¨åˆ é™¤
      await client.hdel('apikey:hash_map', keyData.apiKey)
    }

    const deletedRedis = (await client.del(key)) + (await client.del(legacyKey))

    let deletedPostgres = 0
    if (config.postgres?.enabled) {
      try {
        deletedPostgres = (await postgresStore.deleteApiKeyById(keyId)) ? 1 : 0
      } catch (error) {
        logger.warn(`âš ï¸ Failed to delete API key from PostgreSQL: ${error.message}`)
      }
    }

    return deletedRedis + deletedPostgres
  }

  async getAllApiKeys() {
    const keyIds = await this.scanApiKeyIds()
    return await this.batchGetApiKeys(keyIds, { parse: false })
  }

  /**
   * ä½¿ç”¨ SCAN è·å–æ‰€æœ‰ API Key IDï¼ˆé¿å… KEYS å‘½ä»¤é˜»å¡ï¼‰
   * @returns {Promise<string[]>} API Key ID åˆ—è¡¨
   */
  async scanApiKeyIds() {
    const client = this.getClientSafe()

    // âœ… PostgreSQLï¼ˆå¯é€‰ï¼‰ï¼šä¼˜å…ˆå–ä¸€ä»½ keyIdsï¼Œå…¼å®¹ Redis flush/è¿ç§»åœºæ™¯
    const keyIdSet = new Set()
    if (config.postgres?.enabled) {
      try {
        const pgIds = await postgresStore.listApiKeyIds()
        if (Array.isArray(pgIds)) {
          pgIds.filter(Boolean).forEach((id) => keyIdSet.add(String(id)))
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to list API keys from PostgreSQL: ${error.message}`)
      }
    }

    // ğŸš€ ä¼˜å…ˆä½¿ç”¨ hash_map è·å– keyIdsï¼ˆé¿å…åœ¨å¤§ keyspace ä¸‹å…¨é‡ SCAN å¸¦æ¥çš„è¶…é«˜ RTTï¼‰
    // hash_map: hashedKey -> keyIdï¼Œå•æ¬¡ HVALS å°±èƒ½æ‹¿åˆ°å…¨éƒ¨ keyIdï¼ˆå»é‡åè¿”å›ï¼‰
    try {
      const mappedIds = await client.hvals('apikey:hash_map')
      if (Array.isArray(mappedIds) && mappedIds.length > 0) {
        mappedIds.filter(Boolean).forEach((id) => keyIdSet.add(String(id)))
      }
    } catch (error) {
      // hash_map ä¸å¯ç”¨æ—¶å›é€€åˆ° SCAN
    }

    const keyIds = []
    let cursor = '0'

    do {
      const [newCursor, keys] = await client.scan(cursor, 'MATCH', 'apikey:*', 'COUNT', 20000)
      cursor = newCursor

      for (const key of keys) {
        if (key !== 'apikey:hash_map') {
          keyIds.push(key.replace('apikey:', ''))
        }
      }
    } while (cursor !== '0')

    // å…¼å®¹å†å²å‰ç¼€ï¼ˆä»…ç”¨äºè¯»å–/è¿ç§»ï¼Œä¸ä½œä¸ºä¸»è·¯å¾„ï¼‰
    cursor = '0'
    do {
      const [newCursor, keys] = await client.scan(cursor, 'MATCH', 'api_key:*', 'COUNT', 20000)
      cursor = newCursor
      for (const key of keys) {
        keyIds.push(key.replace('api_key:', ''))
      }
    } while (cursor !== '0')

    keyIds.filter(Boolean).forEach((id) => keyIdSet.add(String(id)))

    return [...keyIdSet]
  }

  /**
   * æ‰¹é‡è·å– API Key æ•°æ®ï¼ˆä½¿ç”¨ Pipeline ä¼˜åŒ–ï¼‰
   * @param {string[]} keyIds - API Key ID åˆ—è¡¨
   * @param {{parse?: boolean, chunkSize?: number, fields?: string[] | null}} options
   * @returns {Promise<Object[]>} API Key æ•°æ®åˆ—è¡¨
   */
  async batchGetApiKeys(keyIds, options = {}) {
    const { parse = true, chunkSize = 500, fields = null } = options
    if (!keyIds || keyIds.length === 0) {
      return []
    }

    const useFields = Array.isArray(fields) && fields.length > 0
    const apiKeys = []

    for (let offset = 0; offset < keyIds.length; offset += chunkSize) {
      const chunkIds = keyIds.slice(offset, offset + chunkSize)
      const client = this.getClientSafe()

      // âœ… è¯»è·¯å¾„ï¼ˆé˜¶æ®µ3ï¼‰ï¼šä¼˜å…ˆ PostgreSQLï¼ˆè·¨èŠ‚ç‚¹ä¸€è‡´æ€§ï¼‰â†’ miss å†å›é€€ Redisï¼ˆå…¼å®¹è¿ç§»æœŸ/PGå¼‚å¸¸ï¼‰
      let pgDataById = new Map()
      if (config.postgres?.enabled) {
        try {
          const pgRows = await postgresStore.getApiKeysByIds(chunkIds)
          if (Array.isArray(pgRows)) {
            pgDataById = new Map(
              pgRows
                .filter((row) => row && row.id && row.data)
                .map((row) => [String(row.id), row.data])
            )
          }
        } catch (error) {
          logger.warn(`âš ï¸ Failed to batch read API keys from PostgreSQL: ${error.message}`)
        }
      }

      const redisIds = chunkIds.filter((id) => !pgDataById.has(String(id)))
      const redisDataById = new Map()

      if (redisIds.length > 0) {
        // ä¼˜å…ˆæ–°å‰ç¼€
        const pipeline = client.pipeline()
        for (const keyId of redisIds) {
          if (useFields) {
            pipeline.hmget(`apikey:${keyId}`, ...fields)
          } else {
            pipeline.hgetall(`apikey:${keyId}`)
          }
        }
        const results = await pipeline.exec()

        const missingLegacyIds = []
        for (let i = 0; i < results.length; i++) {
          const keyId = redisIds[i]
          const [err, data] = results[i]
          if (err) {
            missingLegacyIds.push(keyId)
            continue
          }

          if (useFields) {
            const values = Array.isArray(data) ? data : []
            const mapped = {}
            for (let j = 0; j < fields.length; j++) {
              const value = values[j]
              if (value !== null && value !== undefined) {
                mapped[fields[j]] = value
              }
            }
            if (Object.keys(mapped).length === 0) {
              missingLegacyIds.push(keyId)
              continue
            }
            redisDataById.set(keyId, mapped)
            continue
          }

          if (data && Object.keys(data).length > 0) {
            redisDataById.set(keyId, data)
          } else {
            missingLegacyIds.push(keyId)
          }
        }

        // å…¼å®¹å†å²å‰ç¼€ï¼ˆä»…åœ¨æ–°å‰ç¼€ miss æ—¶è¯»å–ï¼‰
        if (missingLegacyIds.length > 0) {
          const legacyPipeline = client.pipeline()
          for (const keyId of missingLegacyIds) {
            if (useFields) {
              legacyPipeline.hmget(`api_key:${keyId}`, ...fields)
            } else {
              legacyPipeline.hgetall(`api_key:${keyId}`)
            }
          }
          const legacyResults = await legacyPipeline.exec()
          for (let i = 0; i < legacyResults.length; i++) {
            const keyId = missingLegacyIds[i]
            const [err, data] = legacyResults[i]
            if (err) {
              continue
            }

            if (useFields) {
              const values = Array.isArray(data) ? data : []
              const mapped = {}
              for (let j = 0; j < fields.length; j++) {
                const value = values[j]
                if (value !== null && value !== undefined) {
                  mapped[fields[j]] = value
                }
              }
              if (Object.keys(mapped).length === 0) {
                continue
              }
              redisDataById.set(keyId, mapped)
              continue
            }

            if (data && Object.keys(data).length > 0) {
              redisDataById.set(keyId, data)
            }
          }
        }
      }

      for (const keyId of chunkIds) {
        let data = null

        if (pgDataById.has(String(keyId))) {
          const pgFull = pgDataById.get(String(keyId))

          if (useFields) {
            const mapped = {}
            for (const field of fields) {
              const value = pgFull?.[field]
              if (value !== null && value !== undefined) {
                mapped[field] = value
              }
            }
            data = mapped
          } else {
            data = pgFull
          }
        } else {
          data = redisDataById.get(keyId) || null
        }

        if (useFields) {
          apiKeys.push({
            id: keyId,
            ...(parse ? this._parseApiKeyData(data || {}) : data || {})
          })
          continue
        }

        if (data && Object.keys(data).length > 0) {
          apiKeys.push({
            id: keyId,
            ...(parse ? this._parseApiKeyData(data) : data)
          })
        }
      }
    }

    return apiKeys
  }

  /**
   * è§£æ API Key æ•°æ®ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ­£ç¡®çš„ç±»å‹
   * @param {Object} data - åŸå§‹æ•°æ®
   * @returns {Object} è§£æåçš„æ•°æ®
   */
  _parseApiKeyData(data) {
    if (!data) {
      return data
    }

    const parsed = { ...data }

    // å¸ƒå°”å­—æ®µ
    const boolFields = ['isActive', 'enableModelRestriction', 'isDeleted']
    for (const field of boolFields) {
      if (parsed[field] !== undefined) {
        parsed[field] = parsed[field] === 'true'
      }
    }

    // æ•°å­—å­—æ®µ
    const numFields = [
      'tokenLimit',
      'dailyCostLimit',
      'totalCostLimit',
      'rateLimitRequests',
      'rateLimitTokens',
      'rateLimitWindow',
      'rateLimitCost',
      'maxConcurrency',
      'activationDuration'
    ]
    for (const field of numFields) {
      if (parsed[field] !== undefined && parsed[field] !== '') {
        parsed[field] = parseFloat(parsed[field]) || 0
      }
    }

    // æ•°ç»„å­—æ®µï¼ˆJSON è§£æï¼‰
    const arrayFields = ['tags', 'restrictedModels', 'allowedClients']
    for (const field of arrayFields) {
      if (parsed[field]) {
        try {
          parsed[field] = JSON.parse(parsed[field])
        } catch (e) {
          parsed[field] = []
        }
      }
    }

    return parsed
  }

  /**
   * è·å– API Keys åˆ†é¡µæ•°æ®ï¼ˆä¸å«è´¹ç”¨ï¼Œç”¨äºä¼˜åŒ–åˆ—è¡¨åŠ è½½ï¼‰
   * @param {Object} options - åˆ†é¡µå’Œç­›é€‰é€‰é¡¹
   * @returns {Promise<{items: Object[], pagination: Object, availableTags: string[]}>}
   */
  async getApiKeysPaginated(options = {}) {
    const {
      page = 1,
      pageSize = 20,
      searchMode = 'apiKey',
      search = '',
      tag = '',
      isActive = '',
      sortBy = 'createdAt',
      sortOrder = 'desc',
      excludeDeleted = true, // é»˜è®¤æ’é™¤å·²åˆ é™¤çš„ API Keys
      modelFilter = []
    } = options

    // 1. ä½¿ç”¨ SCAN è·å–æ‰€æœ‰ apikey:* çš„ ID åˆ—è¡¨ï¼ˆé¿å…é˜»å¡ï¼‰
    const keyIds = await this.scanApiKeyIds()

    // 2. å…ˆç”¨ HMGET æ‹‰å–â€œåˆ—è¡¨æ‰€éœ€å­—æ®µâ€ï¼ˆé¿å…æŠŠ icon/description ç­‰å¤§å­—æ®µå…¨é‡æ‹‰å›æ¥ï¼‰
    const metaFields = [
      'name',
      'createdAt',
      'expiresAt',
      'lastUsedAt',
      'isActive',
      'isDeleted',
      'tags',
      'userId',
      'userUsername',
      'createdBy',
      'claudeAccountId',
      'claudeConsoleAccountId',
      'geminiAccountId',
      'openaiAccountId',
      'azureOpenaiAccountId',
      'bedrockAccountId',
      'droidAccountId',
      'ccrAccountId'
    ]
    const apiKeyMetas = await this.batchGetApiKeys(keyIds, { fields: metaFields })

    // 3. åº”ç”¨ç­›é€‰æ¡ä»¶
    let filteredKeys = apiKeyMetas

    // æ’é™¤å·²åˆ é™¤çš„ API Keysï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
    if (excludeDeleted) {
      filteredKeys = filteredKeys.filter((k) => !k.isDeleted)
    }

    // çŠ¶æ€ç­›é€‰
    if (isActive !== '' && isActive !== undefined && isActive !== null) {
      const activeValue = isActive === 'true' || isActive === true
      filteredKeys = filteredKeys.filter((k) => k.isActive === activeValue)
    }

    // æ ‡ç­¾ç­›é€‰
    if (tag) {
      filteredKeys = filteredKeys.filter((k) => {
        const tags = Array.isArray(k.tags) ? k.tags : []
        return tags.includes(tag)
      })
    }

    // æœç´¢
    if (search) {
      const lowerSearch = search.toLowerCase().trim()
      if (searchMode === 'apiKey') {
        // apiKey æ¨¡å¼ï¼šæœç´¢åç§°å’Œæ‹¥æœ‰è€…ï¼ˆç”¨æˆ·å/åˆ›å»ºè€…ï¼‰
        filteredKeys = filteredKeys.filter((k) => {
          if (k.name && k.name.toLowerCase().includes(lowerSearch)) {
            return true
          }
          if (k.userUsername && k.userUsername.toLowerCase().includes(lowerSearch)) {
            return true
          }
          if (k.createdBy && k.createdBy.toLowerCase().includes(lowerSearch)) {
            return true
          }
          return false
        })
      } else if (searchMode === 'bindingAccount') {
        // bindingAccount æ¨¡å¼ï¼šç›´æ¥åœ¨Rediså±‚å¤„ç†ï¼Œé¿å…è·¯ç”±å±‚åŠ è½½10000æ¡
        const accountNameCacheService = require('../services/accountNameCacheService')
        filteredKeys = accountNameCacheService.searchByBindingAccount(filteredKeys, lowerSearch)
      }
    }

    // æ¨¡å‹ç­›é€‰
    if (modelFilter.length > 0) {
      const keyIdsWithModels = await this.getKeyIdsWithModels(
        filteredKeys.map((k) => k.id),
        modelFilter
      )
      filteredKeys = filteredKeys.filter((k) => keyIdsWithModels.has(k.id))
    }

    // 4. æ’åº
    filteredKeys.sort((a, b) => {
      // status æ’åºå®é™…ä¸Šä½¿ç”¨ isActive å­—æ®µï¼ˆAPI Key æ²¡æœ‰ status å­—æ®µï¼‰
      const effectiveSortBy = sortBy === 'status' ? 'isActive' : sortBy
      let aVal = a[effectiveSortBy]
      let bVal = b[effectiveSortBy]

      // æ—¥æœŸå­—æ®µè½¬æ—¶é—´æˆ³
      if (['createdAt', 'expiresAt', 'lastUsedAt'].includes(effectiveSortBy)) {
        aVal = aVal ? new Date(aVal).getTime() : 0
        bVal = bVal ? new Date(bVal).getTime() : 0
      }

      // å¸ƒå°”å­—æ®µè½¬æ•°å­—
      if (effectiveSortBy === 'isActive') {
        aVal = aVal ? 1 : 0
        bVal = bVal ? 1 : 0
      }

      // å­—ç¬¦ä¸²å­—æ®µ
      if (sortBy === 'name') {
        aVal = (aVal || '').toLowerCase()
        bVal = (bVal || '').toLowerCase()
      }

      if (aVal < bVal) {
        return sortOrder === 'asc' ? -1 : 1
      }
      if (aVal > bVal) {
        return sortOrder === 'asc' ? 1 : -1
      }
      return 0
    })

    // 5. æ”¶é›†æ‰€æœ‰å¯ç”¨æ ‡ç­¾ï¼ˆåœ¨åˆ†é¡µä¹‹å‰ï¼‰
    const allTags = new Set()
    const tagSource = excludeDeleted ? apiKeyMetas.filter((k) => !k.isDeleted) : apiKeyMetas
    for (const key of tagSource) {
      const tags = Array.isArray(key.tags) ? key.tags : []
      tags.forEach((t) => allTags.add(t))
    }
    const availableTags = [...allTags].sort()

    // 6. åˆ†é¡µ
    const total = filteredKeys.length
    const totalPages = Math.ceil(total / pageSize) || 1
    const validPage = Math.min(Math.max(1, page), totalPages)
    const start = (validPage - 1) * pageSize
    const pageMetas = filteredKeys.slice(start, start + pageSize)

    // 7. åªå¯¹â€œå½“å‰é¡µâ€å†æ‹‰å–å®Œæ•´æ•°æ®ï¼Œä¿æŒè¿”å›ç»“æ„ä¸å˜
    const items = await this.batchGetApiKeys(pageMetas.map((k) => k.id))

    return {
      items,
      pagination: {
        page: validPage,
        pageSize,
        total,
        totalPages
      },
      availableTags
    }
  }

  /**
   * è·å– API Key æ¦‚è§ˆç»Ÿè®¡ï¼ˆç”¨äº Dashboard ç­‰ï¼‰
   * ä»…è¯»å–çŠ¶æ€å­—æ®µ + usage æ€»è®¡ï¼Œé¿å… getAllApiKeys çš„ N+1 å’Œå¤§å­—æ®µä¼ è¾“
   * @param {{excludeDeleted?: boolean, chunkSize?: number}} options
   */
  async getApiKeyOverviewStats(options = {}) {
    const { excludeDeleted = true, chunkSize = 300 } = options
    const keyIds = await this.scanApiKeyIds()

    let totalApiKeys = 0
    let activeApiKeys = 0

    let totalRequestsUsed = 0
    let totalInputTokensUsed = 0
    let totalOutputTokensUsed = 0
    let totalCacheCreateTokensUsed = 0
    let totalCacheReadTokensUsed = 0
    let totalAllTokensUsed = 0

    const parseTotalUsage = (data) => {
      const tokens = parseInt(data.totalTokens) || parseInt(data.tokens) || 0
      const inputTokensRaw = parseInt(data.totalInputTokens) || parseInt(data.inputTokens) || 0
      const outputTokensRaw = parseInt(data.totalOutputTokens) || parseInt(data.outputTokens) || 0
      const cacheCreateTokens =
        parseInt(data.totalCacheCreateTokens) || parseInt(data.cacheCreateTokens) || 0
      const cacheReadTokens =
        parseInt(data.totalCacheReadTokens) || parseInt(data.cacheReadTokens) || 0
      const requests = parseInt(data.totalRequests) || parseInt(data.requests) || 0
      let allTokens = parseInt(data.totalAllTokens) || parseInt(data.allTokens) || 0

      let inputTokens = inputTokensRaw
      let outputTokens = outputTokensRaw

      const totalFromParts = inputTokensRaw + outputTokensRaw + cacheCreateTokens + cacheReadTokens
      if (!allTokens && totalFromParts > 0) {
        allTokens = totalFromParts
      }
      if (!allTokens && tokens > 0) {
        allTokens = tokens
      }

      if (inputTokensRaw + outputTokensRaw === 0 && tokens > 0) {
        outputTokens = Math.round(tokens * 0.7)
        inputTokens = Math.round(tokens * 0.3)
      }

      return { requests, inputTokens, outputTokens, cacheCreateTokens, cacheReadTokens, allTokens }
    }

    for (let offset = 0; offset < keyIds.length; offset += chunkSize) {
      const chunkIds = keyIds.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()

      chunkIds.forEach((keyId) => pipeline.hmget(`apikey:${keyId}`, 'isDeleted', 'isActive'))
      chunkIds.forEach((keyId) => pipeline.hgetall(`usage:${keyId}`))

      const results = await pipeline.exec()
      const metaResults = results.slice(0, chunkIds.length)
      const usageResults = results.slice(chunkIds.length)

      let pgDataById = new Map()
      if (config.postgres?.enabled) {
        const missingIds = []
        for (let i = 0; i < chunkIds.length; i++) {
          const metaValues = metaResults[i]?.[1]
          const isDeletedVal = Array.isArray(metaValues) ? metaValues[0] : null
          const isActiveVal = Array.isArray(metaValues) ? metaValues[1] : null
          if (
            isDeletedVal === null ||
            isDeletedVal === undefined ||
            isActiveVal === null ||
            isActiveVal === undefined
          ) {
            missingIds.push(chunkIds[i])
          }
        }

        if (missingIds.length > 0) {
          try {
            const pgRows = await postgresStore.getApiKeysByIds(missingIds)
            if (Array.isArray(pgRows)) {
              pgDataById = new Map(
                pgRows
                  .filter((row) => row && row.id && row.data)
                  .map((row) => [String(row.id), row.data])
              )
            }
          } catch (error) {
            logger.warn(
              `âš ï¸ Failed to batch read API key overview meta from PostgreSQL: ${error.message}`
            )
          }
        }
      }

      const normalizeBoolean = (value) => value === true || value === 'true'

      for (let i = 0; i < chunkIds.length; i++) {
        const metaValues = metaResults[i]?.[1]
        const pgData = pgDataById.get(String(chunkIds[i]))
        const rawIsDeleted =
          Array.isArray(metaValues) && metaValues[0] !== null && metaValues[0] !== undefined
            ? metaValues[0]
            : pgData?.isDeleted
        const isDeleted = normalizeBoolean(rawIsDeleted)
        if (excludeDeleted && isDeleted) {
          continue
        }

        totalApiKeys++

        const rawIsActive =
          Array.isArray(metaValues) && metaValues[1] !== null && metaValues[1] !== undefined
            ? metaValues[1]
            : pgData?.isActive
        const isActive = normalizeBoolean(rawIsActive)
        if (isActive) {
          activeApiKeys++
        }

        const usageData = usageResults[i]?.[1] || {}
        const parsed = parseTotalUsage(usageData)

        totalRequestsUsed += parsed.requests
        totalInputTokensUsed += parsed.inputTokens
        totalOutputTokensUsed += parsed.outputTokens
        totalCacheCreateTokensUsed += parsed.cacheCreateTokens
        totalCacheReadTokensUsed += parsed.cacheReadTokens
        totalAllTokensUsed += parsed.allTokens
      }
    }

    return {
      totalApiKeys,
      activeApiKeys,
      totalRequestsUsed,
      totalTokensUsed: totalAllTokensUsed, // å…¼å®¹æ—§å­—æ®µå
      totalInputTokensUsed,
      totalOutputTokensUsed,
      totalCacheCreateTokensUsed,
      totalCacheReadTokensUsed,
      totalAllTokensUsed
    }
  }

  /**
   * è·å–è´¦æˆ·ç»‘å®šçš„ API Key æ•°é‡ç»Ÿè®¡ï¼ˆè½»é‡ï¼‰
   * @param {{excludeDeleted?: boolean, chunkSize?: number}} options
   */
  async getApiKeyBindingCounts(options = {}) {
    const { excludeDeleted = true } = options
    const rawChunkSize = Number(options.chunkSize)
    const chunkSize = Number.isFinite(rawChunkSize) && rawChunkSize > 0 ? rawChunkSize : 2000
    const rawCacheTtlMs = Number(options.cacheTtlMs)
    const cacheTtlMs = Number.isFinite(rawCacheTtlMs) ? rawCacheTtlMs : 10 * 1000

    const now = Date.now()
    const cacheKey = excludeDeleted ? 'excludeDeleted' : 'includeDeleted'
    const cached = this._apiKeyBindingCountsCache?.[cacheKey]
    if (
      cacheTtlMs > 0 &&
      cached &&
      cached.expiresAt &&
      typeof cached.expiresAt === 'number' &&
      cached.expiresAt > now &&
      cached.value
    ) {
      return cached.value
    }

    const keyIds = await this.scanApiKeyIds()

    const bindingCounts = {
      claudeAccountId: {},
      claudeConsoleAccountId: {},
      geminiAccountId: {},
      openaiAccountId: {},
      azureOpenaiAccountId: {},
      bedrockAccountId: {},
      droidAccountId: {},
      ccrAccountId: {}
    }

    const fields = [
      'isDeleted',
      'claudeAccountId',
      'claudeConsoleAccountId',
      'geminiAccountId',
      'openaiAccountId',
      'azureOpenaiAccountId',
      'bedrockAccountId',
      'droidAccountId',
      'ccrAccountId'
    ]

    for (let offset = 0; offset < keyIds.length; offset += chunkSize) {
      const chunkIds = keyIds.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()
      chunkIds.forEach((keyId) => pipeline.hmget(`apikey:${keyId}`, ...fields))
      const results = await pipeline.exec()

      for (let i = 0; i < chunkIds.length; i++) {
        const values = results[i]?.[1]
        if (!Array.isArray(values)) {
          continue
        }

        const isDeleted = values[0] === 'true'
        if (excludeDeleted && isDeleted) {
          continue
        }

        for (let j = 1; j < fields.length; j++) {
          const field = fields[j]
          const accountId = values[j]
          if (!accountId) {
            continue
          }
          bindingCounts[field][accountId] = (bindingCounts[field][accountId] || 0) + 1
        }
      }
    }

    if (cacheTtlMs > 0) {
      this._apiKeyBindingCountsCache[cacheKey] = {
        expiresAt: now + cacheTtlMs,
        value: bindingCounts
      }
    }

    return bindingCounts
  }

  /**
   * è·å–å½“å‰ç³»ç»Ÿæ‰€æœ‰å¯ç”¨çš„ API Key æ ‡ç­¾
   * @param {boolean} excludeDeleted
   */
  async getApiKeyAvailableTags(excludeDeleted = true) {
    const keyIds = await this.scanApiKeyIds()
    const fields = excludeDeleted ? ['tags', 'isDeleted'] : ['tags']
    const metas = await this.batchGetApiKeys(keyIds, { fields })
    const tags = new Set()

    for (const key of metas) {
      if (excludeDeleted && key.isDeleted) {
        continue
      }
      const keyTags = Array.isArray(key.tags) ? key.tags : []
      keyTags.forEach((t) => {
        if (t && String(t).trim()) {
          tags.add(String(t).trim())
        }
      })
    }

    return [...tags].sort()
  }

  // ğŸ” é€šè¿‡å“ˆå¸Œå€¼æŸ¥æ‰¾API Keyï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
  async findApiKeyByHash(hashedKey) {
    const client = this.getClientSafe()

    // 1) Redis å¿«é€Ÿè·¯å¾„ï¼šhash_map -> keyId -> apikey:{id}
    const keyId = await client.hget('apikey:hash_map', hashedKey)
    if (keyId) {
      const keyData = await client.hgetall(`apikey:${keyId}`)
      if (keyData && Object.keys(keyData).length > 0) {
        return { id: keyId, ...keyData }
      }

      // å¦‚æœæ•°æ®ä¸å­˜åœ¨ï¼Œæ¸…ç†æ˜ å°„è¡¨ï¼ˆé¿å…è„æ˜ å°„å¯¼è‡´æ°¸è¿œ missï¼‰
      await client.hdel('apikey:hash_map', hashedKey)
    }

    // 2) PostgreSQL å›é€€ï¼šç›´æ¥æŒ‰ hashed_key æŸ¥è¯¢ï¼ˆç”¨äº Redis flush/è¿ç§»åœºæ™¯ï¼‰
    if (config.postgres?.enabled) {
      try {
        const pgData = await postgresStore.getApiKeyByHashedKey(hashedKey)
        if (pgData) {
          const resolvedId = pgData.id || keyId
          return resolvedId ? { id: resolvedId, ...pgData } : pgData
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to find API key in PostgreSQL: ${error.message}`)
      }
    }

    return null
  }

  // ğŸ“Š ä½¿ç”¨ç»Ÿè®¡ç›¸å…³æ“ä½œï¼ˆæ”¯æŒç¼“å­˜tokenç»Ÿè®¡å’Œæ¨¡å‹ä¿¡æ¯ï¼‰
  // æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼Œç”¨äºç»Ÿè®¡èšåˆ
  _normalizeModelName(model) {
    if (!model || model === 'unknown') {
      return model
    }

    // å¯¹äºBedrockæ¨¡å‹ï¼Œå»æ‰åŒºåŸŸå‰ç¼€è¿›è¡Œç»Ÿä¸€
    if (model.includes('.anthropic.') || model.includes('.claude')) {
      // åŒ¹é…æ‰€æœ‰AWSåŒºåŸŸæ ¼å¼ï¼šregion.anthropic.model-name-v1:0 -> claude-model-name
      // æ”¯æŒæ‰€æœ‰AWSåŒºåŸŸæ ¼å¼ï¼Œå¦‚ï¼šus-east-1, eu-west-1, ap-southeast-1, ca-central-1ç­‰
      let normalized = model.replace(/^[a-z0-9-]+\./, '') // å»æ‰ä»»ä½•åŒºåŸŸå‰ç¼€ï¼ˆæ›´é€šç”¨ï¼‰
      normalized = normalized.replace('anthropic.', '') // å»æ‰anthropicå‰ç¼€
      normalized = normalized.replace(/-v\d+:\d+$/, '') // å»æ‰ç‰ˆæœ¬åç¼€ï¼ˆå¦‚-v1:0, -v2:1ç­‰ï¼‰
      return normalized
    }

    // å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œå»æ‰å¸¸è§çš„ç‰ˆæœ¬åç¼€
    return model.replace(/-v\d+:\d+$|:latest$/, '')
  }

  async incrementTokenUsage(
    keyId,
    tokens,
    inputTokens = 0,
    outputTokens = 0,
    cacheCreateTokens = 0,
    cacheReadTokens = 0,
    model = 'unknown',
    ephemeral5mTokens = 0, // æ–°å¢ï¼š5åˆ†é’Ÿç¼“å­˜ tokens
    ephemeral1hTokens = 0, // æ–°å¢ï¼š1å°æ—¶ç¼“å­˜ tokens
    isLongContextRequest = false, // æ–°å¢ï¼šæ˜¯å¦ä¸º 1M ä¸Šä¸‹æ–‡è¯·æ±‚ï¼ˆè¶…è¿‡200kï¼‰
    actualModel = null // æ–°å¢ï¼šä¸Šæ¸¸å®é™…ä½¿ç”¨çš„æ¨¡å‹ï¼ˆç”¨äºç®¡ç†å‘˜ç»Ÿè®¡ï¼‰
  ) {
    const key = `usage:${keyId}`
    const now = new Date()
    const today = getDateStringInTimezone(now)
    const tzDate = getDateInTimezone(now)
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`
    const currentHour = `${today}:${String(getHourInTimezone(now)).padStart(2, '0')}` // æ–°å¢å°æ—¶çº§åˆ«

    const daily = `usage:daily:${keyId}:${today}`
    const monthly = `usage:monthly:${keyId}:${currentMonth}`
    const hourly = `usage:hourly:${keyId}:${currentHour}` // æ–°å¢å°æ—¶çº§åˆ«key

    // æ ‡å‡†åŒ–æ¨¡å‹åç”¨äºç»Ÿè®¡èšåˆ
    const normalizedModel = this._normalizeModelName(model)

    // æŒ‰æ¨¡å‹ç»Ÿè®¡çš„é”®
    const modelDaily = `usage:model:daily:${normalizedModel}:${today}`
    const modelMonthly = `usage:model:monthly:${normalizedModel}:${currentMonth}`
    const modelHourly = `usage:model:hourly:${normalizedModel}:${currentHour}` // æ–°å¢æ¨¡å‹å°æ—¶çº§åˆ«

    // API Keyçº§åˆ«çš„æ¨¡å‹ç»Ÿè®¡
    const keyModelDaily = `usage:${keyId}:model:daily:${normalizedModel}:${today}`
    const keyModelMonthly = `usage:${keyId}:model:monthly:${normalizedModel}:${currentMonth}`
    const keyModelHourly = `usage:${keyId}:model:hourly:${normalizedModel}:${currentHour}` // æ–°å¢API Keyæ¨¡å‹å°æ—¶çº§åˆ«

    // æ–°å¢ï¼šç³»ç»Ÿçº§åˆ†é’Ÿç»Ÿè®¡
    const minuteTimestamp = Math.floor(now.getTime() / 60000)
    const systemMinuteKey = `system:metrics:minute:${minuteTimestamp}`

    // æ™ºèƒ½å¤„ç†è¾“å…¥è¾“å‡ºtokenåˆ†é…
    const finalInputTokens = inputTokens || 0
    const finalOutputTokens = outputTokens || (finalInputTokens > 0 ? 0 : tokens)
    const finalCacheCreateTokens = cacheCreateTokens || 0
    const finalCacheReadTokens = cacheReadTokens || 0

    // é‡æ–°è®¡ç®—çœŸå®çš„æ€»tokenæ•°ï¼ˆåŒ…æ‹¬ç¼“å­˜tokenï¼‰
    const totalTokens =
      finalInputTokens + finalOutputTokens + finalCacheCreateTokens + finalCacheReadTokens
    // æ ¸å¿ƒtokenï¼ˆä¸åŒ…æ‹¬ç¼“å­˜ï¼‰- ç”¨äºä¸å†å²æ•°æ®å…¼å®¹
    const coreTokens = finalInputTokens + finalOutputTokens

    // ä½¿ç”¨Pipelineä¼˜åŒ–æ€§èƒ½
    const pipeline = this.client.pipeline()

    // ç°æœ‰çš„ç»Ÿè®¡ä¿æŒä¸å˜
    // æ ¸å¿ƒtokenç»Ÿè®¡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    pipeline.hincrby(key, 'totalTokens', coreTokens)
    pipeline.hincrby(key, 'totalInputTokens', finalInputTokens)
    pipeline.hincrby(key, 'totalOutputTokens', finalOutputTokens)
    // ç¼“å­˜tokenç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
    pipeline.hincrby(key, 'totalCacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(key, 'totalCacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(key, 'totalAllTokens', totalTokens) // åŒ…å«æ‰€æœ‰ç±»å‹çš„æ€»token
    // è¯¦ç»†ç¼“å­˜ç±»å‹ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
    pipeline.hincrby(key, 'totalEphemeral5mTokens', ephemeral5mTokens)
    pipeline.hincrby(key, 'totalEphemeral1hTokens', ephemeral1hTokens)
    // 1M ä¸Šä¸‹æ–‡è¯·æ±‚ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
    if (isLongContextRequest) {
      pipeline.hincrby(key, 'totalLongContextInputTokens', finalInputTokens)
      pipeline.hincrby(key, 'totalLongContextOutputTokens', finalOutputTokens)
      pipeline.hincrby(key, 'totalLongContextRequests', 1)
    }
    // è¯·æ±‚è®¡æ•°
    pipeline.hincrby(key, 'totalRequests', 1)

    // æ¯æ—¥ç»Ÿè®¡
    pipeline.hincrby(daily, 'tokens', coreTokens)
    pipeline.hincrby(daily, 'inputTokens', finalInputTokens)
    pipeline.hincrby(daily, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(daily, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(daily, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(daily, 'allTokens', totalTokens)
    pipeline.hincrby(daily, 'requests', 1)
    // è¯¦ç»†ç¼“å­˜ç±»å‹ç»Ÿè®¡
    pipeline.hincrby(daily, 'ephemeral5mTokens', ephemeral5mTokens)
    pipeline.hincrby(daily, 'ephemeral1hTokens', ephemeral1hTokens)
    // 1M ä¸Šä¸‹æ–‡è¯·æ±‚ç»Ÿè®¡
    if (isLongContextRequest) {
      pipeline.hincrby(daily, 'longContextInputTokens', finalInputTokens)
      pipeline.hincrby(daily, 'longContextOutputTokens', finalOutputTokens)
      pipeline.hincrby(daily, 'longContextRequests', 1)
    }

    // æ¯æœˆç»Ÿè®¡
    pipeline.hincrby(monthly, 'tokens', coreTokens)
    pipeline.hincrby(monthly, 'inputTokens', finalInputTokens)
    pipeline.hincrby(monthly, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(monthly, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(monthly, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(monthly, 'allTokens', totalTokens)
    pipeline.hincrby(monthly, 'requests', 1)
    // è¯¦ç»†ç¼“å­˜ç±»å‹ç»Ÿè®¡
    pipeline.hincrby(monthly, 'ephemeral5mTokens', ephemeral5mTokens)
    pipeline.hincrby(monthly, 'ephemeral1hTokens', ephemeral1hTokens)

    // æŒ‰æ¨¡å‹ç»Ÿè®¡ - æ¯æ—¥
    pipeline.hincrby(modelDaily, 'inputTokens', finalInputTokens)
    pipeline.hincrby(modelDaily, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(modelDaily, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(modelDaily, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(modelDaily, 'allTokens', totalTokens)
    pipeline.hincrby(modelDaily, 'requests', 1)

    // æŒ‰æ¨¡å‹ç»Ÿè®¡ - æ¯æœˆ
    pipeline.hincrby(modelMonthly, 'inputTokens', finalInputTokens)
    pipeline.hincrby(modelMonthly, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(modelMonthly, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(modelMonthly, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(modelMonthly, 'allTokens', totalTokens)
    pipeline.hincrby(modelMonthly, 'requests', 1)

    // API Keyçº§åˆ«çš„æ¨¡å‹ç»Ÿè®¡ - æ¯æ—¥
    pipeline.hincrby(keyModelDaily, 'inputTokens', finalInputTokens)
    pipeline.hincrby(keyModelDaily, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(keyModelDaily, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(keyModelDaily, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(keyModelDaily, 'allTokens', totalTokens)
    pipeline.hincrby(keyModelDaily, 'requests', 1)
    // è¯¦ç»†ç¼“å­˜ç±»å‹ç»Ÿè®¡
    pipeline.hincrby(keyModelDaily, 'ephemeral5mTokens', ephemeral5mTokens)
    pipeline.hincrby(keyModelDaily, 'ephemeral1hTokens', ephemeral1hTokens)

    // API Keyçº§åˆ«çš„æ¨¡å‹ç»Ÿè®¡ - æ¯æœˆ
    pipeline.hincrby(keyModelMonthly, 'inputTokens', finalInputTokens)
    pipeline.hincrby(keyModelMonthly, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(keyModelMonthly, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(keyModelMonthly, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(keyModelMonthly, 'allTokens', totalTokens)
    pipeline.hincrby(keyModelMonthly, 'requests', 1)
    // è¯¦ç»†ç¼“å­˜ç±»å‹ç»Ÿè®¡
    pipeline.hincrby(keyModelMonthly, 'ephemeral5mTokens', ephemeral5mTokens)
    pipeline.hincrby(keyModelMonthly, 'ephemeral1hTokens', ephemeral1hTokens)

    // å°æ—¶çº§åˆ«ç»Ÿè®¡
    pipeline.hincrby(hourly, 'tokens', coreTokens)
    pipeline.hincrby(hourly, 'inputTokens', finalInputTokens)
    pipeline.hincrby(hourly, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(hourly, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(hourly, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(hourly, 'allTokens', totalTokens)
    pipeline.hincrby(hourly, 'requests', 1)

    // æŒ‰æ¨¡å‹ç»Ÿè®¡ - æ¯å°æ—¶
    pipeline.hincrby(modelHourly, 'inputTokens', finalInputTokens)
    pipeline.hincrby(modelHourly, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(modelHourly, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(modelHourly, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(modelHourly, 'allTokens', totalTokens)
    pipeline.hincrby(modelHourly, 'requests', 1)

    // API Keyçº§åˆ«çš„æ¨¡å‹ç»Ÿè®¡ - æ¯å°æ—¶
    pipeline.hincrby(keyModelHourly, 'inputTokens', finalInputTokens)
    pipeline.hincrby(keyModelHourly, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(keyModelHourly, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(keyModelHourly, 'cacheReadTokens', finalCacheReadTokens)
    pipeline.hincrby(keyModelHourly, 'allTokens', totalTokens)
    pipeline.hincrby(keyModelHourly, 'requests', 1)

    // æ–°å¢ï¼šç³»ç»Ÿçº§åˆ†é’Ÿç»Ÿè®¡
    pipeline.hincrby(systemMinuteKey, 'requests', 1)
    pipeline.hincrby(systemMinuteKey, 'totalTokens', totalTokens)
    pipeline.hincrby(systemMinuteKey, 'inputTokens', finalInputTokens)
    pipeline.hincrby(systemMinuteKey, 'outputTokens', finalOutputTokens)
    pipeline.hincrby(systemMinuteKey, 'cacheCreateTokens', finalCacheCreateTokens)
    pipeline.hincrby(systemMinuteKey, 'cacheReadTokens', finalCacheReadTokens)

    // å¦‚æœæœ‰å®é™…æ¨¡å‹ä¸”ä¸è¯·æ±‚æ¨¡å‹ä¸åŒï¼Œé¢å¤–è®°å½•å®é™…æ¨¡å‹çš„ç»Ÿè®¡ï¼ˆç”¨äºç®¡ç†å‘˜ç»Ÿè®¡ï¼‰
    if (actualModel && actualModel !== model) {
      const normalizedActualModel = this._normalizeModelName(actualModel)
      const actualModelDaily = `usage:model:daily:${normalizedActualModel}:${today}`
      const actualModelMonthly = `usage:model:monthly:${normalizedActualModel}:${currentMonth}`
      const actualModelHourly = `usage:model:hourly:${normalizedActualModel}:${currentHour}`
      const keyActualModelDaily = `usage:${keyId}:model:daily:${normalizedActualModel}:${today}`
      const keyActualModelMonthly = `usage:${keyId}:model:monthly:${normalizedActualModel}:${currentMonth}`
      const keyActualModelHourly = `usage:${keyId}:model:hourly:${normalizedActualModel}:${currentHour}`

      // è®°å½•å®é™…æ¨¡å‹çš„ç³»ç»Ÿçº§ç»Ÿè®¡ï¼ˆç”¨äºç®¡ç†ç•Œé¢æŸ¥çœ‹ï¼‰
      pipeline.hincrby(actualModelDaily, 'inputTokens', finalInputTokens)
      pipeline.hincrby(actualModelDaily, 'outputTokens', finalOutputTokens)
      pipeline.hincrby(actualModelDaily, 'cacheCreateTokens', finalCacheCreateTokens)
      pipeline.hincrby(actualModelDaily, 'cacheReadTokens', finalCacheReadTokens)
      pipeline.hincrby(actualModelDaily, 'allTokens', totalTokens)
      pipeline.hincrby(actualModelDaily, 'requests', 1)

      pipeline.hincrby(actualModelMonthly, 'inputTokens', finalInputTokens)
      pipeline.hincrby(actualModelMonthly, 'outputTokens', finalOutputTokens)
      pipeline.hincrby(actualModelMonthly, 'cacheCreateTokens', finalCacheCreateTokens)
      pipeline.hincrby(actualModelMonthly, 'cacheReadTokens', finalCacheReadTokens)
      pipeline.hincrby(actualModelMonthly, 'allTokens', totalTokens)
      pipeline.hincrby(actualModelMonthly, 'requests', 1)

      pipeline.hincrby(actualModelHourly, 'inputTokens', finalInputTokens)
      pipeline.hincrby(actualModelHourly, 'outputTokens', finalOutputTokens)
      pipeline.hincrby(actualModelHourly, 'cacheCreateTokens', finalCacheCreateTokens)
      pipeline.hincrby(actualModelHourly, 'cacheReadTokens', finalCacheReadTokens)
      pipeline.hincrby(actualModelHourly, 'allTokens', totalTokens)
      pipeline.hincrby(actualModelHourly, 'requests', 1)

      // è®°å½• API Key çº§åˆ«çš„å®é™…æ¨¡å‹ç»Ÿè®¡
      pipeline.hincrby(keyActualModelDaily, 'inputTokens', finalInputTokens)
      pipeline.hincrby(keyActualModelDaily, 'outputTokens', finalOutputTokens)
      pipeline.hincrby(keyActualModelDaily, 'cacheCreateTokens', finalCacheCreateTokens)
      pipeline.hincrby(keyActualModelDaily, 'cacheReadTokens', finalCacheReadTokens)
      pipeline.hincrby(keyActualModelDaily, 'allTokens', totalTokens)
      pipeline.hincrby(keyActualModelDaily, 'requests', 1)

      pipeline.hincrby(keyActualModelMonthly, 'inputTokens', finalInputTokens)
      pipeline.hincrby(keyActualModelMonthly, 'outputTokens', finalOutputTokens)
      pipeline.hincrby(keyActualModelMonthly, 'cacheCreateTokens', finalCacheCreateTokens)
      pipeline.hincrby(keyActualModelMonthly, 'cacheReadTokens', finalCacheReadTokens)
      pipeline.hincrby(keyActualModelMonthly, 'allTokens', totalTokens)
      pipeline.hincrby(keyActualModelMonthly, 'requests', 1)

      pipeline.hincrby(keyActualModelHourly, 'inputTokens', finalInputTokens)
      pipeline.hincrby(keyActualModelHourly, 'outputTokens', finalOutputTokens)
      pipeline.hincrby(keyActualModelHourly, 'cacheCreateTokens', finalCacheCreateTokens)
      pipeline.hincrby(keyActualModelHourly, 'cacheReadTokens', finalCacheReadTokens)
      pipeline.hincrby(keyActualModelHourly, 'allTokens', totalTokens)
      pipeline.hincrby(keyActualModelHourly, 'requests', 1)

      // è®¾ç½®å®é™…æ¨¡å‹ç»Ÿè®¡çš„è¿‡æœŸæ—¶é—´
      pipeline.expire(actualModelDaily, 86400 * 32)
      pipeline.expire(actualModelMonthly, 86400 * 365)
      pipeline.expire(actualModelHourly, 86400 * 7)
      pipeline.expire(keyActualModelDaily, 86400 * 32)
      pipeline.expire(keyActualModelMonthly, 86400 * 365)
      pipeline.expire(keyActualModelHourly, 86400 * 7)
    }

    // è®¾ç½®è¿‡æœŸæ—¶é—´
    pipeline.expire(daily, 86400 * 32) // 32å¤©è¿‡æœŸ
    pipeline.expire(monthly, 86400 * 365) // 1å¹´è¿‡æœŸ
    pipeline.expire(hourly, 86400 * 7) // å°æ—¶ç»Ÿè®¡7å¤©è¿‡æœŸ
    pipeline.expire(modelDaily, 86400 * 32) // æ¨¡å‹æ¯æ—¥ç»Ÿè®¡32å¤©è¿‡æœŸ
    pipeline.expire(modelMonthly, 86400 * 365) // æ¨¡å‹æ¯æœˆç»Ÿè®¡1å¹´è¿‡æœŸ
    pipeline.expire(modelHourly, 86400 * 7) // æ¨¡å‹å°æ—¶ç»Ÿè®¡7å¤©è¿‡æœŸ
    pipeline.expire(keyModelDaily, 86400 * 32) // API Keyæ¨¡å‹æ¯æ—¥ç»Ÿè®¡32å¤©è¿‡æœŸ
    pipeline.expire(keyModelMonthly, 86400 * 365) // API Keyæ¨¡å‹æ¯æœˆç»Ÿè®¡1å¹´è¿‡æœŸ
    pipeline.expire(keyModelHourly, 86400 * 7) // API Keyæ¨¡å‹å°æ—¶ç»Ÿè®¡7å¤©è¿‡æœŸ

    // ç³»ç»Ÿçº§åˆ†é’Ÿç»Ÿè®¡çš„è¿‡æœŸæ—¶é—´ï¼ˆçª—å£æ—¶é—´çš„2å€ï¼‰
    const configLocal = require('../../config/config')
    const { metricsWindow } = configLocal.system
    pipeline.expire(systemMinuteKey, metricsWindow * 60 * 2)

    // æ‰§è¡ŒPipeline
    await pipeline.exec()
  }

  // ğŸ“Š è®°å½•è´¦æˆ·çº§åˆ«çš„ä½¿ç”¨ç»Ÿè®¡
  async incrementAccountUsage(
    accountId,
    totalTokens,
    inputTokens = 0,
    outputTokens = 0,
    cacheCreateTokens = 0,
    cacheReadTokens = 0,
    model = 'unknown',
    isLongContextRequest = false
  ) {
    const now = new Date()
    const today = getDateStringInTimezone(now)
    const tzDate = getDateInTimezone(now)
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`
    const currentHour = `${today}:${String(getHourInTimezone(now)).padStart(2, '0')}`

    // è´¦æˆ·çº§åˆ«ç»Ÿè®¡çš„é”®
    const accountKey = `account_usage:${accountId}`
    const accountDaily = `account_usage:daily:${accountId}:${today}`
    const accountMonthly = `account_usage:monthly:${accountId}:${currentMonth}`
    const accountHourly = `account_usage:hourly:${accountId}:${currentHour}`

    // æ ‡å‡†åŒ–æ¨¡å‹åç”¨äºç»Ÿè®¡èšåˆ
    const normalizedModel = this._normalizeModelName(model)

    // è´¦æˆ·æŒ‰æ¨¡å‹ç»Ÿè®¡çš„é”®
    const accountModelDaily = `account_usage:model:daily:${accountId}:${normalizedModel}:${today}`
    const accountModelMonthly = `account_usage:model:monthly:${accountId}:${normalizedModel}:${currentMonth}`
    const accountModelHourly = `account_usage:model:hourly:${accountId}:${normalizedModel}:${currentHour}`

    // å¤„ç†tokenåˆ†é…
    const finalInputTokens = inputTokens || 0
    const finalOutputTokens = outputTokens || 0
    const finalCacheCreateTokens = cacheCreateTokens || 0
    const finalCacheReadTokens = cacheReadTokens || 0
    const actualTotalTokens =
      finalInputTokens + finalOutputTokens + finalCacheCreateTokens + finalCacheReadTokens
    const coreTokens = finalInputTokens + finalOutputTokens

    // è®¡ç®—æœ¬æ¬¡è¯·æ±‚è´¹ç”¨ï¼ˆç”¨äºè´¦æˆ·çº§åˆ«çš„å¿«é€Ÿç»Ÿè®¡ï¼Œé¿å…åˆ—è¡¨é¡µæŒ‰è´¦å·æ‰«ææ¨¡å‹é”®ï¼‰
    let requestCost = 0
    try {
      const CostCalculator = require('../utils/costCalculator')
      const costResult = CostCalculator.calculateCost(
        {
          input_tokens: finalInputTokens,
          output_tokens: finalOutputTokens,
          cache_creation_input_tokens: finalCacheCreateTokens,
          cache_read_input_tokens: finalCacheReadTokens
        },
        normalizedModel
      )

      const rawCost = costResult?.costs?.total
      requestCost = Number.isFinite(rawCost) && rawCost > 0 ? rawCost : 0
    } catch (error) {
      // è´¹ç”¨è®¡ç®—å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
      requestCost = 0
    }

    // æ„å»ºç»Ÿè®¡æ“ä½œæ•°ç»„
    const operations = [
      // è´¦æˆ·æ€»ä½“ç»Ÿè®¡
      this.client.hincrby(accountKey, 'totalTokens', coreTokens),
      this.client.hincrby(accountKey, 'totalInputTokens', finalInputTokens),
      this.client.hincrby(accountKey, 'totalOutputTokens', finalOutputTokens),
      this.client.hincrby(accountKey, 'totalCacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountKey, 'totalCacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountKey, 'totalAllTokens', actualTotalTokens),
      this.client.hincrby(accountKey, 'totalRequests', 1),
      this.client.hincrbyfloat(accountKey, 'totalCost', requestCost),

      // è´¦æˆ·æ¯æ—¥ç»Ÿè®¡
      this.client.hincrby(accountDaily, 'tokens', coreTokens),
      this.client.hincrby(accountDaily, 'inputTokens', finalInputTokens),
      this.client.hincrby(accountDaily, 'outputTokens', finalOutputTokens),
      this.client.hincrby(accountDaily, 'cacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountDaily, 'cacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountDaily, 'allTokens', actualTotalTokens),
      this.client.hincrby(accountDaily, 'requests', 1),
      this.client.hincrbyfloat(accountDaily, 'cost', requestCost),

      // è´¦æˆ·æ¯æœˆç»Ÿè®¡
      this.client.hincrby(accountMonthly, 'tokens', coreTokens),
      this.client.hincrby(accountMonthly, 'inputTokens', finalInputTokens),
      this.client.hincrby(accountMonthly, 'outputTokens', finalOutputTokens),
      this.client.hincrby(accountMonthly, 'cacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountMonthly, 'cacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountMonthly, 'allTokens', actualTotalTokens),
      this.client.hincrby(accountMonthly, 'requests', 1),
      this.client.hincrbyfloat(accountMonthly, 'cost', requestCost),

      // è´¦æˆ·æ¯å°æ—¶ç»Ÿè®¡
      this.client.hincrby(accountHourly, 'tokens', coreTokens),
      this.client.hincrby(accountHourly, 'inputTokens', finalInputTokens),
      this.client.hincrby(accountHourly, 'outputTokens', finalOutputTokens),
      this.client.hincrby(accountHourly, 'cacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountHourly, 'cacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountHourly, 'allTokens', actualTotalTokens),
      this.client.hincrby(accountHourly, 'requests', 1),

      // æ·»åŠ æ¨¡å‹çº§åˆ«çš„æ•°æ®åˆ°hourlyé”®ä¸­ï¼Œä»¥æ”¯æŒä¼šè¯çª—å£çš„ç»Ÿè®¡
      this.client.hincrby(accountHourly, `model:${normalizedModel}:inputTokens`, finalInputTokens),
      this.client.hincrby(
        accountHourly,
        `model:${normalizedModel}:outputTokens`,
        finalOutputTokens
      ),
      this.client.hincrby(
        accountHourly,
        `model:${normalizedModel}:cacheCreateTokens`,
        finalCacheCreateTokens
      ),
      this.client.hincrby(
        accountHourly,
        `model:${normalizedModel}:cacheReadTokens`,
        finalCacheReadTokens
      ),
      this.client.hincrby(accountHourly, `model:${normalizedModel}:allTokens`, actualTotalTokens),
      this.client.hincrby(accountHourly, `model:${normalizedModel}:requests`, 1),

      // è´¦æˆ·æŒ‰æ¨¡å‹ç»Ÿè®¡ - æ¯æ—¥
      this.client.hincrby(accountModelDaily, 'inputTokens', finalInputTokens),
      this.client.hincrby(accountModelDaily, 'outputTokens', finalOutputTokens),
      this.client.hincrby(accountModelDaily, 'cacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountModelDaily, 'cacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountModelDaily, 'allTokens', actualTotalTokens),
      this.client.hincrby(accountModelDaily, 'requests', 1),

      // è´¦æˆ·æŒ‰æ¨¡å‹ç»Ÿè®¡ - æ¯æœˆ
      this.client.hincrby(accountModelMonthly, 'inputTokens', finalInputTokens),
      this.client.hincrby(accountModelMonthly, 'outputTokens', finalOutputTokens),
      this.client.hincrby(accountModelMonthly, 'cacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountModelMonthly, 'cacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountModelMonthly, 'allTokens', actualTotalTokens),
      this.client.hincrby(accountModelMonthly, 'requests', 1),

      // è´¦æˆ·æŒ‰æ¨¡å‹ç»Ÿè®¡ - æ¯å°æ—¶
      this.client.hincrby(accountModelHourly, 'inputTokens', finalInputTokens),
      this.client.hincrby(accountModelHourly, 'outputTokens', finalOutputTokens),
      this.client.hincrby(accountModelHourly, 'cacheCreateTokens', finalCacheCreateTokens),
      this.client.hincrby(accountModelHourly, 'cacheReadTokens', finalCacheReadTokens),
      this.client.hincrby(accountModelHourly, 'allTokens', actualTotalTokens),
      this.client.hincrby(accountModelHourly, 'requests', 1),

      // è®¾ç½®è¿‡æœŸæ—¶é—´
      this.client.expire(accountDaily, 86400 * 32), // 32å¤©è¿‡æœŸ
      this.client.expire(accountMonthly, 86400 * 365), // 1å¹´è¿‡æœŸ
      this.client.expire(accountHourly, 86400 * 7), // 7å¤©è¿‡æœŸ
      this.client.expire(accountModelDaily, 86400 * 32), // 32å¤©è¿‡æœŸ
      this.client.expire(accountModelMonthly, 86400 * 365), // 1å¹´è¿‡æœŸ
      this.client.expire(accountModelHourly, 86400 * 7) // 7å¤©è¿‡æœŸ
    ]

    // å¦‚æœæ˜¯ 1M ä¸Šä¸‹æ–‡è¯·æ±‚ï¼Œæ·»åŠ é¢å¤–çš„ç»Ÿè®¡
    if (isLongContextRequest) {
      operations.push(
        this.client.hincrby(accountKey, 'totalLongContextInputTokens', finalInputTokens),
        this.client.hincrby(accountKey, 'totalLongContextOutputTokens', finalOutputTokens),
        this.client.hincrby(accountKey, 'totalLongContextRequests', 1),
        this.client.hincrby(accountDaily, 'longContextInputTokens', finalInputTokens),
        this.client.hincrby(accountDaily, 'longContextOutputTokens', finalOutputTokens),
        this.client.hincrby(accountDaily, 'longContextRequests', 1)
      )
    }

    await Promise.all(operations)
  }

  /**
   * è·å–ä½¿ç”¨äº†æŒ‡å®šæ¨¡å‹çš„ Key IDsï¼ˆOR é€»è¾‘ï¼‰
   */
  async getKeyIdsWithModels(keyIds, models) {
    if (!keyIds.length || !models.length) {
      return new Set()
    }

    const client = this.getClientSafe()
    const keyIdSet = new Set(keyIds)
    const result = new Set()

    // é€æ¨¡å‹æ‰«æ usage è®°å½•ï¼Œé¿å… keyIdsÃ—models çš„ KEYS ç»„åˆçˆ†ç‚¸
    for (const model of models) {
      const pattern = `usage:*:model:*:${model}:*`
      let cursor = '0'

      do {
        const [nextCursor, keys] = await client.scan(cursor, 'MATCH', pattern, 'COUNT', 1000)
        cursor = nextCursor

        for (const key of keys) {
          const match = key.match(/^usage:([^:]+):model:/)
          if (!match) {
            continue
          }

          const keyId = match[1]
          if (!keyIdSet.has(keyId)) {
            continue
          }

          result.add(keyId)
          if (result.size >= keyIdSet.size) {
            return result
          }
        }
      } while (cursor !== '0')
    }

    return result
  }

  /**
   * è·å–æ‰€æœ‰è¢«ä½¿ç”¨è¿‡çš„æ¨¡å‹åˆ—è¡¨
   */
  async getAllUsedModels() {
    const client = this.getClientSafe()
    const models = new Set()

    // æ‰«ææ‰€æœ‰æ¨¡å‹ä½¿ç”¨è®°å½•
    const pattern = 'usage:*:model:daily:*'
    let cursor = '0'
    do {
      const [nextCursor, keys] = await client.scan(cursor, 'MATCH', pattern, 'COUNT', 1000)
      cursor = nextCursor
      for (const key of keys) {
        // ä» key ä¸­æå–æ¨¡å‹å: usage:{keyId}:model:daily:{model}:{date}
        const match = key.match(/usage:[^:]+:model:daily:([^:]+):/)
        if (match) {
          models.add(match[1])
        }
      }
    } while (cursor !== '0')

    return [...models].sort()
  }

  async getUsageStats(keyId) {
    const totalKey = `usage:${keyId}`
    const today = getDateStringInTimezone()
    const dailyKey = `usage:daily:${keyId}:${today}`
    const tzDate = getDateInTimezone()
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`
    const monthlyKey = `usage:monthly:${keyId}:${currentMonth}`

    const [total, daily, monthly] = await Promise.all([
      this.client.hgetall(totalKey),
      this.client.hgetall(dailyKey),
      this.client.hgetall(monthlyKey)
    ])

    // è·å–API Keyçš„åˆ›å»ºæ—¶é—´æ¥è®¡ç®—å¹³å‡å€¼
    const keyData = await this.client.hgetall(`apikey:${keyId}`)
    const createdAt = keyData.createdAt ? new Date(keyData.createdAt) : new Date()
    const now = new Date()
    const daysSinceCreated = Math.max(1, Math.ceil((now - createdAt) / (1000 * 60 * 60 * 24)))

    const totalTokens = parseInt(total.totalTokens) || 0
    const totalRequests = parseInt(total.totalRequests) || 0

    // è®¡ç®—å¹³å‡RPM (requests per minute) å’Œ TPM (tokens per minute)
    const totalMinutes = Math.max(1, daysSinceCreated * 24 * 60)
    const avgRPM = totalRequests / totalMinutes
    const avgTPM = totalTokens / totalMinutes

    // å¤„ç†æ—§æ•°æ®å…¼å®¹æ€§ï¼ˆæ”¯æŒç¼“å­˜tokenï¼‰
    const handleLegacyData = (data) => {
      // ä¼˜å…ˆä½¿ç”¨total*å­—æ®µï¼ˆå­˜å‚¨æ—¶ä½¿ç”¨çš„å­—æ®µï¼‰
      const tokens = parseInt(data.totalTokens) || parseInt(data.tokens) || 0
      const inputTokens = parseInt(data.totalInputTokens) || parseInt(data.inputTokens) || 0
      const outputTokens = parseInt(data.totalOutputTokens) || parseInt(data.outputTokens) || 0
      const requests = parseInt(data.totalRequests) || parseInt(data.requests) || 0

      // æ–°å¢ç¼“å­˜tokenå­—æ®µ
      const cacheCreateTokens =
        parseInt(data.totalCacheCreateTokens) || parseInt(data.cacheCreateTokens) || 0
      const cacheReadTokens =
        parseInt(data.totalCacheReadTokens) || parseInt(data.cacheReadTokens) || 0
      const allTokens = parseInt(data.totalAllTokens) || parseInt(data.allTokens) || 0

      const totalFromSeparate = inputTokens + outputTokens
      // è®¡ç®—å®é™…çš„æ€»tokensï¼ˆåŒ…å«æ‰€æœ‰ç±»å‹ï¼‰
      const actualAllTokens =
        allTokens || inputTokens + outputTokens + cacheCreateTokens + cacheReadTokens

      if (totalFromSeparate === 0 && tokens > 0) {
        // æ—§æ•°æ®ï¼šæ²¡æœ‰è¾“å…¥è¾“å‡ºåˆ†ç¦»
        return {
          tokens, // ä¿æŒå…¼å®¹æ€§ï¼Œä½†ç»Ÿä¸€ä½¿ç”¨allTokens
          inputTokens: Math.round(tokens * 0.3), // å‡è®¾30%ä¸ºè¾“å…¥
          outputTokens: Math.round(tokens * 0.7), // å‡è®¾70%ä¸ºè¾“å‡º
          cacheCreateTokens: 0, // æ—§æ•°æ®æ²¡æœ‰ç¼“å­˜token
          cacheReadTokens: 0,
          allTokens: tokens, // å¯¹äºæ—§æ•°æ®ï¼ŒallTokensç­‰äºtokens
          requests
        }
      } else {
        // æ–°æ•°æ®æˆ–æ— æ•°æ® - ç»Ÿä¸€ä½¿ç”¨allTokensä½œä¸ºtokensçš„å€¼
        return {
          tokens: actualAllTokens, // ç»Ÿä¸€ä½¿ç”¨allTokensä½œä¸ºæ€»æ•°
          inputTokens,
          outputTokens,
          cacheCreateTokens,
          cacheReadTokens,
          allTokens: actualAllTokens,
          requests
        }
      }
    }

    const totalData = handleLegacyData(total)
    const dailyData = handleLegacyData(daily)
    const monthlyData = handleLegacyData(monthly)

    return {
      total: totalData,
      daily: dailyData,
      monthly: monthlyData,
      averages: {
        rpm: Math.round(avgRPM * 100) / 100, // ä¿ç•™2ä½å°æ•°
        tpm: Math.round(avgTPM * 100) / 100,
        dailyRequests: Math.round((totalRequests / daysSinceCreated) * 100) / 100,
        dailyTokens: Math.round((totalTokens / daysSinceCreated) * 100) / 100
      }
    }
  }

  async addUsageRecord(keyId, record, maxRecords = 200) {
    const listKey = `usage:records:${keyId}`
    const client = this.getClientSafe()

    try {
      await client
        .multi()
        .lpush(listKey, JSON.stringify(record))
        .ltrim(listKey, 0, Math.max(0, maxRecords - 1))
        .expire(listKey, 86400 * 90) // é»˜è®¤ä¿ç•™90å¤©
        .exec()
    } catch (error) {
      logger.error(`âŒ Failed to append usage record for key ${keyId}:`, error)
    }
  }

  async getUsageRecords(keyId, limit = 50) {
    const listKey = `usage:records:${keyId}`
    const client = this.getClient()

    if (!client) {
      return []
    }

    try {
      const rawRecords = await client.lrange(listKey, 0, Math.max(0, limit - 1))
      return rawRecords
        .map((entry) => {
          try {
            return JSON.parse(entry)
          } catch (error) {
            logger.warn('âš ï¸ Failed to parse usage record entry:', error)
            return null
          }
        })
        .filter(Boolean)
    } catch (error) {
      logger.error(`âŒ Failed to load usage records for key ${keyId}:`, error)
      return []
    }
  }

  // ğŸ’° è·å–å½“æ—¥è´¹ç”¨
  async getDailyCost(keyId) {
    const today = getDateStringInTimezone()
    const costKey = `usage:cost:daily:${keyId}:${today}`
    const cost = await this.client.get(costKey)
    const result = parseFloat(cost || 0)
    logger.debug(
      `ğŸ’° Getting daily cost for ${keyId}, date: ${today}, key: ${costKey}, value: ${cost}, result: ${result}`
    )
    return result
  }

  // ğŸ’° å¢åŠ å½“æ—¥è´¹ç”¨
  async incrementDailyCost(keyId, amount) {
    const today = getDateStringInTimezone()
    const tzDate = getDateInTimezone()
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`
    const currentHour = `${today}:${String(getHourInTimezone(new Date())).padStart(2, '0')}`

    const dailyKey = `usage:cost:daily:${keyId}:${today}`
    const monthlyKey = `usage:cost:monthly:${keyId}:${currentMonth}`
    const hourlyKey = `usage:cost:hourly:${keyId}:${currentHour}`
    const totalKey = `usage:cost:total:${keyId}` // æ€»è´¹ç”¨é”® - æ°¸ä¸è¿‡æœŸï¼ŒæŒç»­ç´¯åŠ 

    logger.debug(
      `ğŸ’° Incrementing cost for ${keyId}, amount: $${amount}, date: ${today}, dailyKey: ${dailyKey}`
    )

    const results = await Promise.all([
      this.client.incrbyfloat(dailyKey, amount),
      this.client.incrbyfloat(monthlyKey, amount),
      this.client.incrbyfloat(hourlyKey, amount),
      this.client.incrbyfloat(totalKey, amount), // âœ… ç´¯åŠ åˆ°æ€»è´¹ç”¨ï¼ˆæ°¸ä¸è¿‡æœŸï¼‰
      // è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆæ³¨æ„ï¼štotalKey ä¸è®¾ç½®è¿‡æœŸæ—¶é—´ï¼Œä¿æŒæ°¸ä¹…ç´¯è®¡ï¼‰
      this.client.expire(dailyKey, 86400 * 30), // 30å¤©
      this.client.expire(monthlyKey, 86400 * 90), // 90å¤©
      this.client.expire(hourlyKey, 86400 * 7) // 7å¤©
    ])

    logger.debug(`ğŸ’° Cost incremented successfully, new daily total: $${results[0]}`)
  }

  // ğŸ’° è·å–è´¹ç”¨ç»Ÿè®¡
  async getCostStats(keyId) {
    const today = getDateStringInTimezone()
    const tzDate = getDateInTimezone()
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`
    const currentHour = `${today}:${String(getHourInTimezone(new Date())).padStart(2, '0')}`

    const [daily, monthly, hourly, total] = await Promise.all([
      this.client.get(`usage:cost:daily:${keyId}:${today}`),
      this.client.get(`usage:cost:monthly:${keyId}:${currentMonth}`),
      this.client.get(`usage:cost:hourly:${keyId}:${currentHour}`),
      this.client.get(`usage:cost:total:${keyId}`)
    ])

    return {
      daily: parseFloat(daily || 0),
      monthly: parseFloat(monthly || 0),
      hourly: parseFloat(hourly || 0),
      total: parseFloat(total || 0)
    }
  }

  // ğŸ’° è·å–æœ¬å‘¨ Opus è´¹ç”¨
  async getWeeklyOpusCost(keyId) {
    const currentWeek = getWeekStringInTimezone()
    const costKey = `usage:opus:weekly:${keyId}:${currentWeek}`
    const cost = await this.client.get(costKey)
    const result = parseFloat(cost || 0)
    logger.debug(
      `ğŸ’° Getting weekly Opus cost for ${keyId}, week: ${currentWeek}, key: ${costKey}, value: ${cost}, result: ${result}`
    )
    return result
  }

  // ğŸ’° å¢åŠ æœ¬å‘¨ Opus è´¹ç”¨
  async incrementWeeklyOpusCost(keyId, amount) {
    const currentWeek = getWeekStringInTimezone()
    const weeklyKey = `usage:opus:weekly:${keyId}:${currentWeek}`
    const totalKey = `usage:opus:total:${keyId}`

    logger.debug(
      `ğŸ’° Incrementing weekly Opus cost for ${keyId}, week: ${currentWeek}, amount: $${amount}`
    )

    // ä½¿ç”¨ pipeline æ‰¹é‡æ‰§è¡Œï¼Œæé«˜æ€§èƒ½
    const pipeline = this.client.pipeline()
    pipeline.incrbyfloat(weeklyKey, amount)
    pipeline.incrbyfloat(totalKey, amount)
    // è®¾ç½®å‘¨è´¹ç”¨é”®çš„è¿‡æœŸæ—¶é—´ä¸º 2 å‘¨
    pipeline.expire(weeklyKey, 14 * 24 * 3600)

    const results = await pipeline.exec()
    logger.debug(`ğŸ’° Opus cost incremented successfully, new weekly total: $${results[0][1]}`)
  }

  // ğŸ’° è®¡ç®—è´¦æˆ·çš„æ¯æ—¥è´¹ç”¨ï¼ˆåŸºäºæ¨¡å‹ä½¿ç”¨ï¼‰
  async getAccountDailyCost(accountId) {
    const CostCalculator = require('../utils/costCalculator')
    const today = getDateStringInTimezone()

    // è·å–è´¦æˆ·ä»Šæ—¥æ‰€æœ‰æ¨¡å‹çš„ä½¿ç”¨æ•°æ®
    const pattern = `account_usage:model:daily:${accountId}:*:${today}`
    const modelKeys = await this.scanKeys(pattern)

    if (!modelKeys || modelKeys.length === 0) {
      return 0
    }

    let totalCost = 0

    const chunkSize = 500
    for (let offset = 0; offset < modelKeys.length; offset += chunkSize) {
      const chunkKeys = modelKeys.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()
      chunkKeys.forEach((key) => pipeline.hgetall(key))
      const results = await pipeline.exec()

      for (let i = 0; i < chunkKeys.length; i++) {
        const key = chunkKeys[i]
        const modelUsage = results?.[i]?.[1]
        if (!modelUsage || (!modelUsage.inputTokens && !modelUsage.outputTokens)) {
          continue
        }

        // ä»keyä¸­è§£ææ¨¡å‹åç§°
        // æ ¼å¼ï¼šaccount_usage:model:daily:{accountId}:{model}:{date}
        const parts = key.split(':')
        const model = parts[4] // æ¨¡å‹ååœ¨ç¬¬5ä¸ªä½ç½®ï¼ˆç´¢å¼•4ï¼‰

        const usage = {
          input_tokens: parseInt(modelUsage.inputTokens || 0),
          output_tokens: parseInt(modelUsage.outputTokens || 0),
          cache_creation_input_tokens: parseInt(modelUsage.cacheCreateTokens || 0),
          cache_read_input_tokens: parseInt(modelUsage.cacheReadTokens || 0)
        }

        // ä½¿ç”¨CostCalculatorè®¡ç®—è´¹ç”¨
        const costResult = CostCalculator.calculateCost(usage, model)
        totalCost += costResult.costs.total

        logger.debug(
          `ğŸ’° Account ${accountId} daily cost for model ${model}: $${costResult.costs.total}`
        )
      }
    }

    logger.debug(`ğŸ’° Account ${accountId} total daily cost: $${totalCost}`)
    return totalCost
  }

  // ğŸ“Š è·å–è´¦æˆ·ä½¿ç”¨ç»Ÿè®¡
  async getAccountUsageStats(accountId, accountType = null) {
    const accountKey = `account_usage:${accountId}`
    const today = getDateStringInTimezone()
    const accountDailyKey = `account_usage:daily:${accountId}:${today}`
    const tzDate = getDateInTimezone()
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`
    const accountMonthlyKey = `account_usage:monthly:${accountId}:${currentMonth}`

    const [total, daily, monthly] = await Promise.all([
      this.client.hgetall(accountKey),
      this.client.hgetall(accountDailyKey),
      this.client.hgetall(accountMonthlyKey)
    ])

    // è·å–è´¦æˆ·åˆ›å»ºæ—¶é—´æ¥è®¡ç®—å¹³å‡å€¼ - æ”¯æŒä¸åŒç±»å‹çš„è´¦å·
    let accountData = {}
    if (accountType === 'droid') {
      accountData = await this.client.hgetall(`droid:account:${accountId}`)
    } else if (accountType === 'openai') {
      accountData = await this.client.hgetall(`openai:account:${accountId}`)
    } else if (accountType === 'openai-responses') {
      accountData = await this.client.hgetall(`openai_responses_account:${accountId}`)
    } else {
      // å°è¯•å¤šä¸ªå‰ç¼€
      accountData = await this.client.hgetall(`claude_account:${accountId}`)
      if (!accountData.createdAt) {
        accountData = await this.client.hgetall(`openai:account:${accountId}`)
      }
      if (!accountData.createdAt) {
        accountData = await this.client.hgetall(`openai_responses_account:${accountId}`)
      }
      if (!accountData.createdAt) {
        accountData = await this.client.hgetall(`openai_account:${accountId}`)
      }
      if (!accountData.createdAt) {
        accountData = await this.client.hgetall(`droid:account:${accountId}`)
      }
    }
    const now = new Date()
    const createdAtMs = accountData.createdAt
      ? new Date(accountData.createdAt).getTime()
      : now.getTime()
    const safeCreatedAtMs = Number.isFinite(createdAtMs) ? createdAtMs : now.getTime()
    const daysSinceCreated = Math.max(
      1,
      Math.ceil((now.getTime() - safeCreatedAtMs) / (1000 * 60 * 60 * 24))
    )

    const totalTokens = parseInt(total.totalTokens) || 0
    const totalRequests = parseInt(total.totalRequests) || 0

    // è®¡ç®—å¹³å‡RPMå’ŒTPM
    const totalMinutes = Math.max(1, daysSinceCreated * 24 * 60)
    const avgRPM = totalRequests / totalMinutes
    const avgTPM = totalTokens / totalMinutes

    // å¤„ç†è´¦æˆ·ç»Ÿè®¡æ•°æ®
    const handleAccountData = (data) => {
      const tokens = parseInt(data.totalTokens) || parseInt(data.tokens) || 0
      const inputTokens = parseInt(data.totalInputTokens) || parseInt(data.inputTokens) || 0
      const outputTokens = parseInt(data.totalOutputTokens) || parseInt(data.outputTokens) || 0
      const requests = parseInt(data.totalRequests) || parseInt(data.requests) || 0
      const cacheCreateTokens =
        parseInt(data.totalCacheCreateTokens) || parseInt(data.cacheCreateTokens) || 0
      const cacheReadTokens =
        parseInt(data.totalCacheReadTokens) || parseInt(data.cacheReadTokens) || 0
      const allTokens = parseInt(data.totalAllTokens) || parseInt(data.allTokens) || 0

      const actualAllTokens =
        allTokens || inputTokens + outputTokens + cacheCreateTokens + cacheReadTokens

      return {
        tokens,
        inputTokens,
        outputTokens,
        cacheCreateTokens,
        cacheReadTokens,
        allTokens: actualAllTokens,
        requests
      }
    }

    const totalData = handleAccountData(total)
    const dailyData = handleAccountData(daily)
    const monthlyData = handleAccountData(monthly)

    const totalCost = parseFloat(total.totalCost) || 0
    const dailyCost = parseFloat(daily.cost) || 0
    const monthlyCost = parseFloat(monthly.cost) || 0

    return {
      accountId,
      total: {
        ...totalData,
        cost: totalCost
      },
      daily: {
        ...dailyData,
        cost: dailyCost
      },
      monthly: {
        ...monthlyData,
        cost: monthlyCost
      },
      averages: {
        rpm: Math.round(avgRPM * 100) / 100,
        tpm: Math.round(avgTPM * 100) / 100,
        dailyRequests: Math.round((totalRequests / daysSinceCreated) * 100) / 100,
        dailyTokens: Math.round((totalTokens / daysSinceCreated) * 100) / 100
      }
    }
  }

  // ğŸ“Š æ‰¹é‡è·å–å¤šä¸ªè´¦æˆ·çš„ä½¿ç”¨ç»Ÿè®¡ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼špipeline æ‰¹é‡è¯»å–ï¼‰
  async getAccountsUsageStats(accountIds = [], options = {}) {
    const uniqueAccountIds = [...new Set((accountIds || []).filter(Boolean))]
    const createdAtByAccountId = options.createdAtByAccountId || {}
    const rawChunkSize = Number(options.chunkSize)
    const chunkSize = Number.isFinite(rawChunkSize) && rawChunkSize > 0 ? rawChunkSize : 200

    const resultMap = Object.fromEntries(uniqueAccountIds.map((id) => [id, null]))

    if (uniqueAccountIds.length === 0) {
      return resultMap
    }

    const today = getDateStringInTimezone()
    const tzDate = getDateInTimezone()
    const currentMonth = `${tzDate.getUTCFullYear()}-${String(tzDate.getUTCMonth() + 1).padStart(
      2,
      '0'
    )}`

    const now = new Date()

    const handleAccountData = (data) => {
      const tokens = parseInt(data.totalTokens) || parseInt(data.tokens) || 0
      const inputTokens = parseInt(data.totalInputTokens) || parseInt(data.inputTokens) || 0
      const outputTokens = parseInt(data.totalOutputTokens) || parseInt(data.outputTokens) || 0
      const requests = parseInt(data.totalRequests) || parseInt(data.requests) || 0
      const cacheCreateTokens =
        parseInt(data.totalCacheCreateTokens) || parseInt(data.cacheCreateTokens) || 0
      const cacheReadTokens =
        parseInt(data.totalCacheReadTokens) || parseInt(data.cacheReadTokens) || 0
      const allTokens = parseInt(data.totalAllTokens) || parseInt(data.allTokens) || 0

      const actualAllTokens =
        allTokens || inputTokens + outputTokens + cacheCreateTokens + cacheReadTokens

      return {
        tokens,
        inputTokens,
        outputTokens,
        cacheCreateTokens,
        cacheReadTokens,
        allTokens: actualAllTokens,
        requests
      }
    }

    for (let offset = 0; offset < uniqueAccountIds.length; offset += chunkSize) {
      const chunkAccountIds = uniqueAccountIds.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()

      for (const accountId of chunkAccountIds) {
        pipeline.hgetall(`account_usage:${accountId}`)
        pipeline.hgetall(`account_usage:daily:${accountId}:${today}`)
        pipeline.hgetall(`account_usage:monthly:${accountId}:${currentMonth}`)
      }

      const results = await pipeline.exec()

      for (let i = 0; i < chunkAccountIds.length; i++) {
        const accountId = chunkAccountIds[i]
        const baseIndex = i * 3

        const totalRaw = results?.[baseIndex]?.[1] || {}
        const dailyRaw = results?.[baseIndex + 1]?.[1] || {}
        const monthlyRaw = results?.[baseIndex + 2]?.[1] || {}

        const totalTokens = parseInt(totalRaw.totalTokens) || 0
        const totalRequests = parseInt(totalRaw.totalRequests) || 0

        const createdAtRaw = createdAtByAccountId[accountId]
        const createdAtMs = createdAtRaw ? new Date(createdAtRaw).getTime() : now.getTime()
        const safeCreatedAtMs = Number.isFinite(createdAtMs) ? createdAtMs : now.getTime()
        const daysSinceCreated = Math.max(
          1,
          Math.ceil((now.getTime() - safeCreatedAtMs) / (1000 * 60 * 60 * 24))
        )
        const totalMinutes = Math.max(1, daysSinceCreated * 24 * 60)

        const avgRPM = totalRequests / totalMinutes
        const avgTPM = totalTokens / totalMinutes

        const totalCost = parseFloat(totalRaw.totalCost) || 0
        const dailyCost = parseFloat(dailyRaw.cost) || 0
        const monthlyCost = parseFloat(monthlyRaw.cost) || 0

        const totalData = handleAccountData(totalRaw)
        const dailyData = handleAccountData(dailyRaw)
        const monthlyData = handleAccountData(monthlyRaw)

        resultMap[accountId] = {
          accountId,
          total: {
            ...totalData,
            cost: totalCost
          },
          daily: {
            ...dailyData,
            cost: dailyCost
          },
          monthly: {
            ...monthlyData,
            cost: monthlyCost
          },
          averages: {
            rpm: Math.round(avgRPM * 100) / 100,
            tpm: Math.round(avgTPM * 100) / 100,
            dailyRequests: Math.round((totalRequests / daysSinceCreated) * 100) / 100,
            dailyTokens: Math.round((totalTokens / daysSinceCreated) * 100) / 100
          }
        }
      }
    }

    return resultMap
  }

  // ğŸ“ˆ è·å–æ‰€æœ‰è´¦æˆ·çš„ä½¿ç”¨ç»Ÿè®¡
  async getAllAccountsUsageStats() {
    try {
      // è·å–æ‰€æœ‰Claudeè´¦æˆ·
      const accountKeys = await this.scanKeys('claude_account:*')
      const accountStats = []

      for (const accountKey of accountKeys) {
        const accountId = accountKey.replace('claude_account:', '')
        const accountData = await this.client.hgetall(accountKey)

        if (accountData.name) {
          const stats = await this.getAccountUsageStats(accountId)
          accountStats.push({
            id: accountId,
            name: accountData.name,
            email: accountData.email || '',
            status: accountData.status || 'unknown',
            isActive: accountData.isActive === 'true',
            ...stats
          })
        }
      }

      // æŒ‰å½“æ—¥tokenä½¿ç”¨é‡æ’åº
      accountStats.sort((a, b) => (b.daily.allTokens || 0) - (a.daily.allTokens || 0))

      return accountStats
    } catch (error) {
      logger.error('âŒ Failed to get all accounts usage stats:', error)
      return []
    }
  }

  // ğŸ§¹ æ¸…ç©ºæ‰€æœ‰API Keyçš„ä½¿ç”¨ç»Ÿè®¡æ•°æ®
  async resetAllUsageStats() {
    const client = this.getClientSafe()
    const stats = {
      deletedKeys: 0,
      deletedDailyKeys: 0,
      deletedMonthlyKeys: 0,
      resetApiKeys: 0
    }

    try {
      // è·å–æ‰€æœ‰API Key ID
      const apiKeyIds = await this.scanApiKeyIds()

      // æ¸…ç©ºæ¯ä¸ªAPI Keyçš„ä½¿ç”¨ç»Ÿè®¡
      for (const keyId of apiKeyIds) {
        // åˆ é™¤æ€»ä½“ä½¿ç”¨ç»Ÿè®¡
        const usageKey = `usage:${keyId}`
        const deleted = await client.del(usageKey)
        if (deleted > 0) {
          stats.deletedKeys++
        }

        // åˆ é™¤è¯¥API Keyçš„æ¯æ—¥ç»Ÿè®¡ï¼ˆä½¿ç”¨ç²¾ç¡®çš„keyIdåŒ¹é…ï¼‰
        const dailyKeys = await this.scanKeys(`usage:daily:${keyId}:*`)
        if (dailyKeys.length > 0) {
          await client.del(...dailyKeys)
          stats.deletedDailyKeys += dailyKeys.length
        }

        // åˆ é™¤è¯¥API Keyçš„æ¯æœˆç»Ÿè®¡ï¼ˆä½¿ç”¨ç²¾ç¡®çš„keyIdåŒ¹é…ï¼‰
        const monthlyKeys = await this.scanKeys(`usage:monthly:${keyId}:*`)
        if (monthlyKeys.length > 0) {
          await client.del(...monthlyKeys)
          stats.deletedMonthlyKeys += monthlyKeys.length
        }

        // é‡ç½®API Keyçš„lastUsedAtå­—æ®µ
        const keyData = await client.hgetall(`apikey:${keyId}`)
        if (keyData && Object.keys(keyData).length > 0) {
          keyData.lastUsedAt = ''
          await this.setApiKey(keyId, keyData)
          stats.resetApiKeys++
        }
      }

      // é¢å¤–æ¸…ç†ï¼šåˆ é™¤æ‰€æœ‰å¯èƒ½é—æ¼çš„usageç›¸å…³é”®
      const allUsageKeys = await this.scanKeys('usage:*')
      if (allUsageKeys.length > 0) {
        await client.del(...allUsageKeys)
        stats.deletedKeys += allUsageKeys.length
      }

      return stats
    } catch (error) {
      throw new Error(`Failed to reset usage stats: ${error.message}`)
    }
  }

  // ğŸ¢ Claude è´¦æˆ·ç®¡ç†
  async setClaudeAccount(accountId, accountData) {
    const client = this.getClientSafe()
    const key = `claude:account:${accountId}`
    await client.hset(key, accountData)

    if (config.postgres?.enabled) {
      try {
        await postgresStore.upsertAccount('claude', accountId, { id: accountId, ...accountData })
      } catch (error) {
        logger.warn(`âš ï¸ Failed to upsert Claude account into PostgreSQL: ${error.message}`)
      }
    }
  }

  async getClaudeAccount(accountId) {
    const client = this.getClientSafe()
    const key = `claude:account:${accountId}`

    if (config.postgres?.enabled) {
      try {
        const pgData = await postgresStore.getAccount('claude', accountId)
        if (pgData) {
          return pgData
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to read Claude account from PostgreSQL: ${error.message}`)
      }
    }

    const data = await client.hgetall(key)
    if (data && Object.keys(data).length > 0) {
      return data
    }

    return {}
  }

  async getAllClaudeAccounts() {
    if (config.postgres?.enabled) {
      try {
        const pgAccounts = await postgresStore.listAccounts('claude')
        if (Array.isArray(pgAccounts) && pgAccounts.length > 0) {
          return pgAccounts
            .map((account) => {
              const id = account?.id || account?.accountId
              return id ? { id: String(id), ...account } : account
            })
            .filter(Boolean)
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to list Claude accounts from PostgreSQL: ${error.message}`)
      }
    }

    const keys = await this.scanKeys('claude:account:*')

    const accounts = []
    const chunkSize = 300

    for (let offset = 0; offset < keys.length; offset += chunkSize) {
      const chunkKeys = keys.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()
      chunkKeys.forEach((key) => pipeline.hgetall(key))

      const results = await pipeline.exec()
      for (let i = 0; i < results.length; i++) {
        const [err, accountData] = results[i]
        if (!err && accountData && Object.keys(accountData).length > 0) {
          const key = chunkKeys[i]
          accounts.push({ id: key.replace('claude:account:', ''), ...accountData })
        }
      }
    }

    return accounts
  }

  async deleteClaudeAccount(accountId) {
    const client = this.getClientSafe()
    const key = `claude:account:${accountId}`
    const deletedRedis = await client.del(key)
    let deletedPostgres = 0
    if (config.postgres?.enabled) {
      try {
        deletedPostgres = (await postgresStore.deleteAccount('claude', accountId)) ? 1 : 0
      } catch (error) {
        logger.warn(`âš ï¸ Failed to delete Claude account from PostgreSQL: ${error.message}`)
      }
    }
    return deletedRedis + deletedPostgres
  }

  // ğŸ¤– Droid è´¦æˆ·ç›¸å…³æ“ä½œ
  async setDroidAccount(accountId, accountData) {
    const client = this.getClientSafe()
    const key = `droid:account:${accountId}`
    await client.hset(key, accountData)

    if (config.postgres?.enabled) {
      try {
        await postgresStore.upsertAccount('droid', accountId, { id: accountId, ...accountData })
      } catch (error) {
        logger.warn(`âš ï¸ Failed to upsert Droid account into PostgreSQL: ${error.message}`)
      }
    }
  }

  async getDroidAccount(accountId) {
    const client = this.getClientSafe()
    const key = `droid:account:${accountId}`

    if (config.postgres?.enabled) {
      try {
        const pgData = await postgresStore.getAccount('droid', accountId)
        if (pgData) {
          return pgData
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to read Droid account from PostgreSQL: ${error.message}`)
      }
    }

    const data = await client.hgetall(key)
    if (data && Object.keys(data).length > 0) {
      return data
    }

    return {}
  }

  async getAllDroidAccounts() {
    if (config.postgres?.enabled) {
      try {
        const pgAccounts = await postgresStore.listAccounts('droid')
        if (Array.isArray(pgAccounts) && pgAccounts.length > 0) {
          return pgAccounts
            .map((account) => {
              const id = account?.id || account?.accountId
              return id ? { id: String(id), ...account } : account
            })
            .filter(Boolean)
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to list Droid accounts from PostgreSQL: ${error.message}`)
      }
    }

    const keys = await this.scanKeys('droid:account:*')

    const accounts = []
    const chunkSize = 300

    for (let offset = 0; offset < keys.length; offset += chunkSize) {
      const chunkKeys = keys.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()
      chunkKeys.forEach((key) => pipeline.hgetall(key))

      const results = await pipeline.exec()
      for (let i = 0; i < results.length; i++) {
        const [err, accountData] = results[i]
        if (!err && accountData && Object.keys(accountData).length > 0) {
          const key = chunkKeys[i]
          accounts.push({ id: key.replace('droid:account:', ''), ...accountData })
        }
      }
    }

    return accounts
  }

  async deleteDroidAccount(accountId) {
    const client = this.getClientSafe()
    const key = `droid:account:${accountId}`
    const deletedRedis = await client.del(key)
    let deletedPostgres = 0
    if (config.postgres?.enabled) {
      try {
        deletedPostgres = (await postgresStore.deleteAccount('droid', accountId)) ? 1 : 0
      } catch (error) {
        logger.warn(`âš ï¸ Failed to delete Droid account from PostgreSQL: ${error.message}`)
      }
    }
    return deletedRedis + deletedPostgres
  }

  async setOpenAiAccount(accountId, accountData) {
    const client = this.getClientSafe()
    const key = `openai:account:${accountId}`
    await client.hset(key, accountData)

    if (config.postgres?.enabled) {
      try {
        await postgresStore.upsertAccount('openai', accountId, { id: accountId, ...accountData })
      } catch (error) {
        logger.warn(`âš ï¸ Failed to upsert OpenAI account into PostgreSQL: ${error.message}`)
      }
    }
  }
  async getOpenAiAccount(accountId) {
    const client = this.getClientSafe()
    const key = `openai:account:${accountId}`

    if (config.postgres?.enabled) {
      try {
        const pgData = await postgresStore.getAccount('openai', accountId)
        if (pgData) {
          return pgData
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to read OpenAI account from PostgreSQL: ${error.message}`)
      }
    }

    const data = await client.hgetall(key)
    if (data && Object.keys(data).length > 0) {
      return data
    }

    return {}
  }
  async deleteOpenAiAccount(accountId) {
    const client = this.getClientSafe()
    const key = `openai:account:${accountId}`
    const deletedRedis = await client.del(key)
    let deletedPostgres = 0
    if (config.postgres?.enabled) {
      try {
        deletedPostgres = (await postgresStore.deleteAccount('openai', accountId)) ? 1 : 0
      } catch (error) {
        logger.warn(`âš ï¸ Failed to delete OpenAI account from PostgreSQL: ${error.message}`)
      }
    }
    return deletedRedis + deletedPostgres
  }

  async getAllOpenAIAccounts() {
    if (config.postgres?.enabled) {
      try {
        const pgAccounts = await postgresStore.listAccounts('openai')
        if (Array.isArray(pgAccounts) && pgAccounts.length > 0) {
          return pgAccounts
            .map((account) => {
              const id = account?.id || account?.accountId
              return id ? { id: String(id), ...account } : account
            })
            .filter(Boolean)
        }
      } catch (error) {
        logger.warn(`âš ï¸ Failed to list OpenAI accounts from PostgreSQL: ${error.message}`)
      }
    }

    const keys = await this.scanKeys('openai:account:*')

    const accounts = []
    const chunkSize = 300

    for (let offset = 0; offset < keys.length; offset += chunkSize) {
      const chunkKeys = keys.slice(offset, offset + chunkSize)
      const pipeline = this.client.pipeline()
      chunkKeys.forEach((key) => pipeline.hgetall(key))

      const results = await pipeline.exec()
      for (let i = 0; i < results.length; i++) {
        const [err, accountData] = results[i]
        if (!err && accountData && Object.keys(accountData).length > 0) {
          const key = chunkKeys[i]
          accounts.push({ id: key.replace('openai:account:', ''), ...accountData })
        }
      }
    }

    return accounts
  }

  // ğŸ” ä¼šè¯ç®¡ç†ï¼ˆç”¨äºç®¡ç†å‘˜ç™»å½•ç­‰ï¼‰
  async setSession(sessionId, sessionData, ttl = 86400) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        // Go æœåŠ¡çš„ Session ç»“æ„æ˜¯ { token, userId, data, createdAt, expiresAt }
        // Node ä¾§å†å²ä¸Šç›´æ¥å­˜ Hashï¼Œå› æ­¤è¿™é‡Œç»Ÿä¸€æŠŠ sessionData æ”¾åˆ° data å­—æ®µé‡Œï¼Œä¿æŒè°ƒç”¨æ–¹è¯­ä¹‰ä¸å˜
        await goRedisProxy.setSession(sessionId, { data: sessionData }, ttl)
        return
      } catch (error) {
        logger.warn(`âš ï¸ Go service setSession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `session:${sessionId}`
    try {
      await this.client.hset(key, sessionData)
      await this.client.expire(key, ttl)
    } catch (error) {
      // å¦‚æœä¹‹å‰ç”± Go æœåŠ¡å†™å…¥äº†å­—ç¬¦ä¸²ç±»å‹çš„ session keyï¼Œè¿™é‡Œä¼šè§¦å‘ WRONGTYPE
      if (String(error?.message || '').includes('WRONGTYPE')) {
        await this.client.del(key)
        await this.client.hset(key, sessionData)
        await this.client.expire(key, ttl)
        return
      }
      throw error
    }
  }

  async getSession(sessionId) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        const session = await goRedisProxy.getSession(sessionId)
        if (session && typeof session === 'object' && session.data && typeof session.data === 'object') {
          return session.data
        }
        // å…œåº•ï¼šè¿”å›ç©ºå¯¹è±¡ä»¥ä¿æŒ hgetall è¯­ä¹‰
        return {}
      } catch (error) {
        logger.warn(`âš ï¸ Go service getSession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `session:${sessionId}`
    try {
      return await this.client.hgetall(key)
    } catch (error) {
      // Go æœåŠ¡å­˜å‚¨ä¸ºå­—ç¬¦ä¸² JSONï¼ŒRedis å›é€€è¯»å– hash ä¼šè§¦å‘ WRONGTYPE
      if (String(error?.message || '').includes('WRONGTYPE')) {
        const raw = await this.client.get(key)
        if (!raw) {
          return {}
        }
        try {
          const parsed = JSON.parse(raw)
          return parsed && typeof parsed === 'object' && parsed.data && typeof parsed.data === 'object'
            ? parsed.data
            : {}
        } catch (_e) {
          return {}
        }
      }
      throw error
    }
  }

  async deleteSession(sessionId) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        await goRedisProxy.deleteSession(sessionId)
        return 1
      } catch (error) {
        logger.warn(`âš ï¸ Go service deleteSession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `session:${sessionId}`
    return await this.client.del(key)
  }

  // ğŸ—ï¸ API Keyå“ˆå¸Œç´¢å¼•ç®¡ç†
  async setApiKeyHash(hashedKey, keyData, ttl = 0) {
    const key = `apikey_hash:${hashedKey}`
    await this.client.hset(key, keyData)
    if (ttl > 0) {
      await this.client.expire(key, ttl)
    }
  }

  async getApiKeyHash(hashedKey) {
    const key = `apikey_hash:${hashedKey}`
    return await this.client.hgetall(key)
  }

  async deleteApiKeyHash(hashedKey) {
    const key = `apikey_hash:${hashedKey}`
    return await this.client.del(key)
  }

  // ğŸ”— OAuthä¼šè¯ç®¡ç†
  async setOAuthSession(sessionId, sessionData, ttl = 600) {
    // åºåˆ—åŒ–å¤æ‚å¯¹è±¡ï¼Œç‰¹åˆ«æ˜¯ proxy é…ç½®
    const serializedData = {}
    for (const [dataKey, value] of Object.entries(sessionData)) {
      if (typeof value === 'object' && value !== null) {
        serializedData[dataKey] = JSON.stringify(value)
      } else {
        serializedData[dataKey] = value
      }
    }

    // OAuth ä¼šè¯å½“å‰ä»ä»¥ Node.js ä¾§æ•°æ®ç»“æ„ä¸ºå‡†ï¼ˆåŒ…å« state/codeChallenge/proxy ç­‰å­—æ®µï¼‰ï¼Œ
    // Go ä¾§å®ç°å°šæœªä¸è¯¥ç»“æ„å®Œå…¨å¯¹é½ï¼Œå› æ­¤é»˜è®¤ä¸èµ° Go Proxyï¼Œé¿å…æˆæƒæµç¨‹å¼‚å¸¸ã€‚
    const useGoOAuthProxy = process.env.GO_REDIS_PROXY_OAUTH_ENABLED === 'true'
    if (useGoOAuthProxy && (await goRedisProxy.isAvailable())) {
      try {
        await goRedisProxy.setOAuthSession(sessionId, serializedData)
        return
      } catch (error) {
        logger.warn(`âš ï¸ Go service setOAuthSession failed, falling back to Redis: ${error.message}`)
      }
    }

    // 10åˆ†é’Ÿè¿‡æœŸ
    const key = `oauth:${sessionId}`
    await this.client.hset(key, serializedData)
    await this.client.expire(key, ttl)
  }

  async getOAuthSession(sessionId) {
    let data = null

    // é»˜è®¤ä¸èµ° Go OAuth Proxyï¼ŒåŸå› åŒ setOAuthSession
    const useGoOAuthProxy = process.env.GO_REDIS_PROXY_OAUTH_ENABLED === 'true'
    if (useGoOAuthProxy && (await goRedisProxy.isAvailable())) {
      try {
        data = await goRedisProxy.getOAuthSession(sessionId)
      } catch (error) {
        logger.warn(`âš ï¸ Go service getOAuthSession failed, falling back to Redis: ${error.message}`)
      }
    }

    // å›é€€åˆ°ç›´æ¥ Redis
    if (!data) {
      const key = `oauth:${sessionId}`
      data = await this.client.hgetall(key)
    }

    // ååºåˆ—åŒ– proxy å­—æ®µ
    if (data && data.proxy) {
      try {
        data.proxy = JSON.parse(data.proxy)
      } catch (error) {
        // å¦‚æœè§£æå¤±è´¥ï¼Œè®¾ç½®ä¸º null
        data.proxy = null
      }
    }

    return data
  }

  async deleteOAuthSession(sessionId) {
    // é»˜è®¤ä¸èµ° Go OAuth Proxyï¼ŒåŸå› åŒ setOAuthSession
    const useGoOAuthProxy = process.env.GO_REDIS_PROXY_OAUTH_ENABLED === 'true'
    if (useGoOAuthProxy && (await goRedisProxy.isAvailable())) {
      try {
        await goRedisProxy.deleteOAuthSession(sessionId)
        return 1
      } catch (error) {
        logger.warn(`âš ï¸ Go service deleteOAuthSession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `oauth:${sessionId}`
    return await this.client.del(key)
  }

  // ğŸ“ˆ ç³»ç»Ÿç»Ÿè®¡
  async getSystemStats() {
    const [totalApiKeys, totalClaudeAccounts, totalUsageRecords] = await Promise.all([
      this.countKeysByScan('apikey:*', (key) => key !== 'apikey:hash_map'),
      this.countKeysByScan('claude:account:*'),
      this.countKeysByScan('usage:*')
    ])

    return { totalApiKeys, totalClaudeAccounts, totalUsageRecords }
  }

  // ğŸ“Š è·å–ä»Šæ—¥ç³»ç»Ÿç»Ÿè®¡
  async getTodayStats() {
    try {
      const today = getDateStringInTimezone()
      const dailyKeys = await this.scanKeys(`usage:daily:*:${today}`)

      let totalRequestsToday = 0
      let totalTokensToday = 0
      let totalInputTokensToday = 0
      let totalOutputTokensToday = 0
      let totalCacheCreateTokensToday = 0
      let totalCacheReadTokensToday = 0

      // æ‰¹é‡è·å–æ‰€æœ‰ä»Šæ—¥æ•°æ®ï¼Œæé«˜æ€§èƒ½
      if (dailyKeys.length > 0) {
        const pipeline = this.client.pipeline()
        dailyKeys.forEach((key) => pipeline.hgetall(key))
        const results = await pipeline.exec()

        for (const [error, dailyData] of results) {
          if (error || !dailyData) {
            continue
          }

          totalRequestsToday += parseInt(dailyData.requests) || 0
          const currentDayTokens = parseInt(dailyData.tokens) || 0
          totalTokensToday += currentDayTokens

          // å¤„ç†æ—§æ•°æ®å…¼å®¹æ€§ï¼šå¦‚æœæœ‰æ€»tokenä½†æ²¡æœ‰è¾“å…¥è¾“å‡ºåˆ†ç¦»ï¼Œåˆ™ä½¿ç”¨æ€»tokenä½œä¸ºè¾“å‡ºtoken
          const inputTokens = parseInt(dailyData.inputTokens) || 0
          const outputTokens = parseInt(dailyData.outputTokens) || 0
          const cacheCreateTokens = parseInt(dailyData.cacheCreateTokens) || 0
          const cacheReadTokens = parseInt(dailyData.cacheReadTokens) || 0
          const totalTokensFromSeparate = inputTokens + outputTokens

          if (totalTokensFromSeparate === 0 && currentDayTokens > 0) {
            // æ—§æ•°æ®ï¼šæ²¡æœ‰è¾“å…¥è¾“å‡ºåˆ†ç¦»ï¼Œå‡è®¾70%ä¸ºè¾“å‡ºï¼Œ30%ä¸ºè¾“å…¥ï¼ˆåŸºäºä¸€èˆ¬å¯¹è¯æ¯”ä¾‹ï¼‰
            totalOutputTokensToday += Math.round(currentDayTokens * 0.7)
            totalInputTokensToday += Math.round(currentDayTokens * 0.3)
          } else {
            // æ–°æ•°æ®ï¼šä½¿ç”¨å®é™…çš„è¾“å…¥è¾“å‡ºåˆ†ç¦»
            totalInputTokensToday += inputTokens
            totalOutputTokensToday += outputTokens
          }

          // æ·»åŠ cache tokenç»Ÿè®¡
          totalCacheCreateTokensToday += cacheCreateTokens
          totalCacheReadTokensToday += cacheReadTokens
        }
      }

      // è·å–ä»Šæ—¥åˆ›å»ºçš„API Keyæ•°é‡ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰
      const allApiKeys = await this.scanKeys('apikey:*')
      let apiKeysCreatedToday = 0

      if (allApiKeys.length > 0) {
        const pipeline = this.client.pipeline()
        allApiKeys.forEach((key) => {
          if (key !== 'apikey:hash_map') {
            pipeline.hget(key, 'createdAt')
          }
        })
        const results = await pipeline.exec()

        for (const [error, createdAt] of results) {
          if (!error && createdAt && createdAt.startsWith(today)) {
            apiKeysCreatedToday++
          }
        }
      }

      return {
        requestsToday: totalRequestsToday,
        tokensToday: totalTokensToday,
        inputTokensToday: totalInputTokensToday,
        outputTokensToday: totalOutputTokensToday,
        cacheCreateTokensToday: totalCacheCreateTokensToday,
        cacheReadTokensToday: totalCacheReadTokensToday,
        apiKeysCreatedToday
      }
    } catch (error) {
      console.error('Error getting today stats:', error)
      return {
        requestsToday: 0,
        tokensToday: 0,
        inputTokensToday: 0,
        outputTokensToday: 0,
        cacheCreateTokensToday: 0,
        cacheReadTokensToday: 0,
        apiKeysCreatedToday: 0
      }
    }
  }

  // ğŸ“ˆ è·å–ç³»ç»Ÿæ€»çš„å¹³å‡RPMå’ŒTPM
  async getSystemAverages() {
    try {
      const allApiKeys = (await this.scanKeys('apikey:*')).filter(
        (key) => key !== 'apikey:hash_map'
      )
      let totalRequests = 0
      let totalTokens = 0
      let totalInputTokens = 0
      let totalOutputTokens = 0
      let oldestCreatedAt = new Date()

      // æ‰¹é‡è·å–æ‰€æœ‰usageæ•°æ®å’Œ createdAtï¼Œæé«˜æ€§èƒ½ï¼ˆåˆ†æ‰¹é¿å…è¶…å¤§ pipelineï¼‰
      const chunkSize = 300
      for (let offset = 0; offset < allApiKeys.length; offset += chunkSize) {
        const chunkKeys = allApiKeys.slice(offset, offset + chunkSize)
        const pipeline = this.client.pipeline()

        chunkKeys.forEach((key) => pipeline.hgetall(`usage:${key.replace('apikey:', '')}`))
        chunkKeys.forEach((key) => pipeline.hget(key, 'createdAt'))

        const results = await pipeline.exec()
        const usageResults = results.slice(0, chunkKeys.length)
        const createdAtResults = results.slice(chunkKeys.length)

        for (let i = 0; i < chunkKeys.length; i++) {
          const totalData = usageResults[i][1] || {}
          const createdAtValue = createdAtResults[i][1] || ''

          totalRequests += parseInt(totalData.totalRequests) || 0
          totalTokens += parseInt(totalData.totalTokens) || 0
          totalInputTokens += parseInt(totalData.totalInputTokens) || 0
          totalOutputTokens += parseInt(totalData.totalOutputTokens) || 0

          const createdAt = createdAtValue ? new Date(createdAtValue) : new Date()
          if (createdAt < oldestCreatedAt) {
            oldestCreatedAt = createdAt
          }
        }
      }

      const now = new Date()
      // ä¿æŒä¸ä¸ªäººAPI Keyè®¡ç®—ä¸€è‡´çš„ç®—æ³•ï¼šæŒ‰å¤©è®¡ç®—ç„¶åè½¬æ¢ä¸ºåˆ†é’Ÿ
      const daysSinceOldest = Math.max(
        1,
        Math.ceil((now - oldestCreatedAt) / (1000 * 60 * 60 * 24))
      )
      const totalMinutes = daysSinceOldest * 24 * 60

      return {
        systemRPM: Math.round((totalRequests / totalMinutes) * 100) / 100,
        systemTPM: Math.round((totalTokens / totalMinutes) * 100) / 100,
        totalInputTokens,
        totalOutputTokens,
        totalTokens
      }
    } catch (error) {
      console.error('Error getting system averages:', error)
      return {
        systemRPM: 0,
        systemTPM: 0,
        totalInputTokens: 0,
        totalOutputTokens: 0,
        totalTokens: 0
      }
    }
  }

  // ğŸ“Š è·å–å®æ—¶ç³»ç»ŸæŒ‡æ ‡ï¼ˆåŸºäºæ»‘åŠ¨çª—å£ï¼‰
  async getRealtimeSystemMetrics() {
    try {
      const configLocal = require('../../config/config')
      const windowMinutes = configLocal.system.metricsWindow || 5

      const now = new Date()
      const currentMinute = Math.floor(now.getTime() / 60000)

      // è°ƒè¯•ï¼šæ‰“å°å½“å‰æ—¶é—´å’Œåˆ†é’Ÿæ—¶é—´æˆ³
      logger.debug(
        `ğŸ” Realtime metrics - Current time: ${now.toISOString()}, Minute timestamp: ${currentMinute}`
      )

      // ä½¿ç”¨Pipelineæ‰¹é‡è·å–çª—å£å†…çš„æ‰€æœ‰åˆ†é’Ÿæ•°æ®
      const pipeline = this.client.pipeline()
      const minuteKeys = []
      for (let i = 0; i < windowMinutes; i++) {
        const minuteKey = `system:metrics:minute:${currentMinute - i}`
        minuteKeys.push(minuteKey)
        pipeline.hgetall(minuteKey)
      }

      logger.debug(`ğŸ” Realtime metrics - Checking keys: ${minuteKeys.join(', ')}`)

      const results = await pipeline.exec()

      // èšåˆè®¡ç®—
      let totalRequests = 0
      let totalTokens = 0
      let totalInputTokens = 0
      let totalOutputTokens = 0
      let totalCacheCreateTokens = 0
      let totalCacheReadTokens = 0
      let validDataCount = 0

      results.forEach(([err, data], index) => {
        if (!err && data && Object.keys(data).length > 0) {
          validDataCount++
          totalRequests += parseInt(data.requests || 0)
          totalTokens += parseInt(data.totalTokens || 0)
          totalInputTokens += parseInt(data.inputTokens || 0)
          totalOutputTokens += parseInt(data.outputTokens || 0)
          totalCacheCreateTokens += parseInt(data.cacheCreateTokens || 0)
          totalCacheReadTokens += parseInt(data.cacheReadTokens || 0)

          logger.debug(`ğŸ” Realtime metrics - Key ${minuteKeys[index]} data:`, {
            requests: data.requests,
            totalTokens: data.totalTokens
          })
        }
      })

      logger.debug(
        `ğŸ” Realtime metrics - Valid data count: ${validDataCount}/${windowMinutes}, Total requests: ${totalRequests}, Total tokens: ${totalTokens}`
      )

      // è®¡ç®—å¹³å‡å€¼ï¼ˆæ¯åˆ†é’Ÿï¼‰
      const realtimeRPM =
        windowMinutes > 0 ? Math.round((totalRequests / windowMinutes) * 100) / 100 : 0
      const realtimeTPM =
        windowMinutes > 0 ? Math.round((totalTokens / windowMinutes) * 100) / 100 : 0

      const result = {
        realtimeRPM,
        realtimeTPM,
        windowMinutes,
        totalRequests,
        totalTokens,
        totalInputTokens,
        totalOutputTokens,
        totalCacheCreateTokens,
        totalCacheReadTokens
      }

      logger.debug('ğŸ” Realtime metrics - Final result:', result)

      return result
    } catch (error) {
      console.error('Error getting realtime system metrics:', error)
      // å¦‚æœå‡ºé”™ï¼Œè¿”å›å†å²å¹³å‡å€¼ä½œä¸ºé™çº§æ–¹æ¡ˆ
      const historicalMetrics = await this.getSystemAverages()
      return {
        realtimeRPM: historicalMetrics.systemRPM,
        realtimeTPM: historicalMetrics.systemTPM,
        windowMinutes: 0, // æ ‡è¯†ä½¿ç”¨äº†å†å²æ•°æ®
        totalRequests: 0,
        totalTokens: historicalMetrics.totalTokens,
        totalInputTokens: historicalMetrics.totalInputTokens,
        totalOutputTokens: historicalMetrics.totalOutputTokens,
        totalCacheCreateTokens: 0,
        totalCacheReadTokens: 0
      }
    }
  }

  // ğŸ”— ä¼šè¯stickyæ˜ å°„ç®¡ç†
  async setSessionAccountMapping(sessionHash, accountId, ttl = null) {
    const appConfig = require('../../config/config')
    // ä»é…ç½®è¯»å–TTLï¼ˆå°æ—¶ï¼‰ï¼Œè½¬æ¢ä¸ºç§’ï¼Œé»˜è®¤1å°æ—¶
    const defaultTTL = ttl !== null ? ttl : (appConfig.session?.stickyTtlHours || 1) * 60 * 60

    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        await goRedisProxy.setStickySession(sessionHash, accountId, 'unknown', defaultTTL)
        return
      } catch (error) {
        logger.warn(`âš ï¸ Go service setStickySession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `sticky_session:${sessionHash}`
    await this.client.set(key, accountId, 'EX', defaultTTL)
  }

  async getSessionAccountMapping(sessionHash) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        const result = await goRedisProxy.getStickySession(sessionHash)
        return result?.accountId || null
      } catch (error) {
        logger.warn(`âš ï¸ Go service getStickySession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `sticky_session:${sessionHash}`
    return await this.client.get(key)
  }

  // ğŸš€ æ™ºèƒ½ä¼šè¯TTLç»­æœŸï¼šå‰©ä½™æ—¶é—´å°‘äºé˜ˆå€¼æ—¶è‡ªåŠ¨ç»­æœŸ
  async extendSessionAccountMappingTTL(sessionHash) {
    const appConfig = require('../../config/config')
    const key = `sticky_session:${sessionHash}`

    // ğŸ“Š ä»é…ç½®è·å–å‚æ•°
    const ttlHours = appConfig.session?.stickyTtlHours || 1 // å°æ—¶ï¼Œé»˜è®¤1å°æ—¶
    const thresholdMinutes = appConfig.session?.renewalThresholdMinutes || 0 // åˆ†é’Ÿï¼Œé»˜è®¤0ï¼ˆä¸ç»­æœŸï¼‰

    // å¦‚æœé˜ˆå€¼ä¸º0ï¼Œä¸æ‰§è¡Œç»­æœŸ
    if (thresholdMinutes === 0) {
      return true
    }

    const fullTTL = ttlHours * 60 * 60 // è½¬æ¢ä¸ºç§’
    const renewalThreshold = thresholdMinutes * 60 // è½¬æ¢ä¸ºç§’

    try {
      // è·å–å½“å‰å‰©ä½™TTLï¼ˆç§’ï¼‰
      const remainingTTL = await this.client.ttl(key)

      // é”®ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ
      if (remainingTTL === -2) {
        return false
      }

      // é”®å­˜åœ¨ä½†æ²¡æœ‰TTLï¼ˆæ°¸ä¸è¿‡æœŸï¼Œä¸éœ€è¦å¤„ç†ï¼‰
      if (remainingTTL === -1) {
        return true
      }

      // ğŸ¯ æ™ºèƒ½ç»­æœŸç­–ç•¥ï¼šä»…åœ¨å‰©ä½™æ—¶é—´å°‘äºé˜ˆå€¼æ—¶æ‰ç»­æœŸ
      if (remainingTTL < renewalThreshold) {
        await this.client.expire(key, fullTTL)
        logger.debug(
          `ğŸ”„ Renewed sticky session TTL: ${sessionHash} (was ${Math.round(
            remainingTTL / 60
          )}min, renewed to ${ttlHours}h)`
        )
        return true
      }

      // å‰©ä½™æ—¶é—´å……è¶³ï¼Œæ— éœ€ç»­æœŸ
      logger.debug(
        `âœ… Sticky session TTL sufficient: ${sessionHash} (remaining ${Math.round(
          remainingTTL / 60
        )}min)`
      )
      return true
    } catch (error) {
      logger.error('âŒ Failed to extend session TTL:', error)
      return false
    }
  }

  async deleteSessionAccountMapping(sessionHash) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        await goRedisProxy.deleteStickySession(sessionHash)
        return 1
      } catch (error) {
        logger.warn(`âš ï¸ Go service deleteStickySession failed, falling back to Redis: ${error.message}`)
      }
    }

    const key = `sticky_session:${sessionHash}`
    return await this.client.del(key)
  }

  // ğŸ§¹ æ¸…ç†è¿‡æœŸæ•°æ®
  async cleanup() {
    try {
      const patterns = ['usage:daily:*', 'ratelimit:*', 'session:*', 'sticky_session:*', 'oauth:*']

      for (const pattern of patterns) {
        const keys = await this.scanKeys(pattern)
        if (keys.length === 0) {
          continue
        }

        const chunkSize = 1000
        for (let offset = 0; offset < keys.length; offset += chunkSize) {
          const chunkKeys = keys.slice(offset, offset + chunkSize)

          const ttlPipeline = this.client.pipeline()
          chunkKeys.forEach((key) => ttlPipeline.ttl(key))
          const ttlResults = await ttlPipeline.exec()

          const expirePipeline = this.client.pipeline()
          for (let i = 0; i < chunkKeys.length; i++) {
            const ttl = ttlResults?.[i]?.[1]
            if (ttl === -1) {
              // æ²¡æœ‰è®¾ç½®è¿‡æœŸæ—¶é—´çš„é”®
              const key = chunkKeys[i]
              if (key.startsWith('oauth:')) {
                expirePipeline.expire(key, 600) // OAuthä¼šè¯è®¾ç½®10åˆ†é’Ÿè¿‡æœŸ
              } else {
                expirePipeline.expire(key, 86400) // å…¶ä»–è®¾ç½®1å¤©è¿‡æœŸ
              }
            }
          }

          await expirePipeline.exec()
        }
      }

      logger.info('ğŸ§¹ Redis cleanup completed')
    } catch (error) {
      logger.error('âŒ Redis cleanup failed:', error)
    }
  }

  // è·å–å¹¶å‘é…ç½®
  _getConcurrencyConfig() {
    const defaults = {
      leaseSeconds: 300,
      renewIntervalSeconds: 30,
      cleanupGraceSeconds: 30
    }

    const configValues = {
      ...defaults,
      ...(config.concurrency || {})
    }

    const normalizeNumber = (value, fallback, options = {}) => {
      const parsed = Number(value)
      if (!Number.isFinite(parsed)) {
        return fallback
      }

      if (options.allowZero && parsed === 0) {
        return 0
      }

      if (options.min !== undefined && parsed < options.min) {
        return options.min
      }

      return parsed
    }

    return {
      leaseSeconds: normalizeNumber(configValues.leaseSeconds, defaults.leaseSeconds, {
        min: 30
      }),
      renewIntervalSeconds: normalizeNumber(
        configValues.renewIntervalSeconds,
        defaults.renewIntervalSeconds,
        {
          allowZero: true,
          min: 0
        }
      ),
      cleanupGraceSeconds: normalizeNumber(
        configValues.cleanupGraceSeconds,
        defaults.cleanupGraceSeconds,
        {
          min: 0
        }
      )
    }
  }

  // å¢åŠ å¹¶å‘è®¡æ•°ï¼ˆåŸºäºç§Ÿçº¦çš„æœ‰åºé›†åˆï¼‰
  async incrConcurrency(apiKeyId, requestId, leaseSeconds = null) {
    if (!requestId) {
      throw new Error('Request ID is required for concurrency tracking')
    }

    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        const { leaseSeconds: defaultLeaseSeconds } = this._getConcurrencyConfig()
        const lease = leaseSeconds || defaultLeaseSeconds
        const count = await goRedisProxy.incrConcurrency(apiKeyId, requestId, lease)
        logger.database(
          `ğŸ”¢ [Go] Incremented concurrency for key ${apiKeyId}: ${count} (request ${requestId})`
        )
        return count
      } catch (error) {
        logger.warn(`âš ï¸ Go service incrConcurrency failed, falling back to Redis: ${error.message}`)
      }
    }

    // å›é€€åˆ°ç›´æ¥ Redis æ“ä½œ
    try {
      const { leaseSeconds: defaultLeaseSeconds, cleanupGraceSeconds } =
        this._getConcurrencyConfig()
      const lease = leaseSeconds || defaultLeaseSeconds
      const key = `concurrency:${apiKeyId}`
      const now = Date.now()
      const expireAt = now + lease * 1000
      const ttl = Math.max((lease + cleanupGraceSeconds) * 1000, 60000)

      const luaScript = `
        local key = KEYS[1]
        local member = ARGV[1]
        local expireAt = tonumber(ARGV[2])
        local now = tonumber(ARGV[3])
        local ttl = tonumber(ARGV[4])

        redis.call('ZREMRANGEBYSCORE', key, '-inf', now)
        redis.call('ZADD', key, expireAt, member)

        if ttl > 0 then
          redis.call('PEXPIRE', key, ttl)
        end

        local count = redis.call('ZCARD', key)
        return count
      `

      const count = await this.client.eval(luaScript, 1, key, requestId, expireAt, now, ttl)
      logger.database(
        `ğŸ”¢ Incremented concurrency for key ${apiKeyId}: ${count} (request ${requestId})`
      )
      return count
    } catch (error) {
      logger.error('âŒ Failed to increment concurrency:', error)
      throw error
    }
  }

  // åˆ·æ–°å¹¶å‘ç§Ÿçº¦ï¼Œé˜²æ­¢é•¿è¿æ¥æå‰è¿‡æœŸ
  async refreshConcurrencyLease(apiKeyId, requestId, leaseSeconds = null) {
    if (!requestId) {
      return 0
    }

    try {
      const { leaseSeconds: defaultLeaseSeconds, cleanupGraceSeconds } =
        this._getConcurrencyConfig()
      const lease = leaseSeconds || defaultLeaseSeconds
      const key = `concurrency:${apiKeyId}`
      const now = Date.now()
      const expireAt = now + lease * 1000
      const ttl = Math.max((lease + cleanupGraceSeconds) * 1000, 60000)

      const luaScript = `
        local key = KEYS[1]
        local member = ARGV[1]
        local expireAt = tonumber(ARGV[2])
        local now = tonumber(ARGV[3])
        local ttl = tonumber(ARGV[4])

        redis.call('ZREMRANGEBYSCORE', key, '-inf', now)

        local exists = redis.call('ZSCORE', key, member)

        if exists then
          redis.call('ZADD', key, expireAt, member)
          if ttl > 0 then
            redis.call('PEXPIRE', key, ttl)
          end
          return 1
        end

        return 0
      `

      const refreshed = await this.client.eval(luaScript, 1, key, requestId, expireAt, now, ttl)
      if (refreshed === 1) {
        logger.debug(`ğŸ”„ Refreshed concurrency lease for key ${apiKeyId} (request ${requestId})`)
      }
      return refreshed
    } catch (error) {
      logger.error('âŒ Failed to refresh concurrency lease:', error)
      return 0
    }
  }

  // å‡å°‘å¹¶å‘è®¡æ•°
  async decrConcurrency(apiKeyId, requestId) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        const count = await goRedisProxy.decrConcurrency(apiKeyId, requestId)
        logger.database(
          `ğŸ”¢ [Go] Decremented concurrency for key ${apiKeyId}: ${count} (request ${requestId || 'n/a'})`
        )
        return count
      } catch (error) {
        logger.warn(`âš ï¸ Go service decrConcurrency failed, falling back to Redis: ${error.message}`)
      }
    }

    // å›é€€åˆ°ç›´æ¥ Redis æ“ä½œ
    try {
      const key = `concurrency:${apiKeyId}`
      const now = Date.now()

      const luaScript = `
        local key = KEYS[1]
        local member = ARGV[1]
        local now = tonumber(ARGV[2])

        if member then
          redis.call('ZREM', key, member)
        end

        redis.call('ZREMRANGEBYSCORE', key, '-inf', now)

        local count = redis.call('ZCARD', key)
        if count <= 0 then
          redis.call('DEL', key)
          return 0
        end

        return count
      `

      const count = await this.client.eval(luaScript, 1, key, requestId || '', now)
      logger.database(
        `ğŸ”¢ Decremented concurrency for key ${apiKeyId}: ${count} (request ${requestId || 'n/a'})`
      )
      return count
    } catch (error) {
      logger.error('âŒ Failed to decrement concurrency:', error)
      throw error
    }
  }

  // è·å–å½“å‰å¹¶å‘æ•°
  async getConcurrency(apiKeyId) {
    // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
    if (await goRedisProxy.isAvailable()) {
      try {
        return await goRedisProxy.getConcurrency(apiKeyId)
      } catch (error) {
        logger.warn(`âš ï¸ Go service getConcurrency failed, falling back to Redis: ${error.message}`)
      }
    }

    // å›é€€åˆ°ç›´æ¥ Redis æ“ä½œ
    try {
      const key = `concurrency:${apiKeyId}`
      const now = Date.now()

      const luaScript = `
        local key = KEYS[1]
        local now = tonumber(ARGV[1])

        redis.call('ZREMRANGEBYSCORE', key, '-inf', now)
        return redis.call('ZCARD', key)
      `

      const count = await this.client.eval(luaScript, 1, key, now)
      return parseInt(count || 0)
    } catch (error) {
      logger.error('âŒ Failed to get concurrency:', error)
      return 0
    }
  }

  // ğŸ¢ Claude Console è´¦æˆ·å¹¶å‘æ§åˆ¶ï¼ˆå¤ç”¨ç°æœ‰å¹¶å‘æœºåˆ¶ï¼‰
  // å¢åŠ  Console è´¦æˆ·å¹¶å‘è®¡æ•°
  async incrConsoleAccountConcurrency(accountId, requestId, leaseSeconds = null) {
    if (!requestId) {
      throw new Error('Request ID is required for console account concurrency tracking')
    }
    // ä½¿ç”¨ç‰¹æ®Šçš„ key å‰ç¼€åŒºåˆ† Console è´¦æˆ·å¹¶å‘
    const compositeKey = `console_account:${accountId}`
    return await this.incrConcurrency(compositeKey, requestId, leaseSeconds)
  }

  // åˆ·æ–° Console è´¦æˆ·å¹¶å‘ç§Ÿçº¦
  async refreshConsoleAccountConcurrencyLease(accountId, requestId, leaseSeconds = null) {
    if (!requestId) {
      return 0
    }
    const compositeKey = `console_account:${accountId}`
    return await this.refreshConcurrencyLease(compositeKey, requestId, leaseSeconds)
  }

  // å‡å°‘ Console è´¦æˆ·å¹¶å‘è®¡æ•°
  async decrConsoleAccountConcurrency(accountId, requestId) {
    const compositeKey = `console_account:${accountId}`
    return await this.decrConcurrency(compositeKey, requestId)
  }

  // è·å– Console è´¦æˆ·å½“å‰å¹¶å‘æ•°
  async getConsoleAccountConcurrency(accountId) {
    const compositeKey = `console_account:${accountId}`
    return await this.getConcurrency(compositeKey)
  }

  // ğŸ”§ å¹¶å‘ç®¡ç†æ–¹æ³•ï¼ˆç”¨äºç®¡ç†å‘˜æ‰‹åŠ¨æ¸…ç†ï¼‰

  /**
   * è·å–æ‰€æœ‰å¹¶å‘çŠ¶æ€
   * @returns {Promise<Array>} å¹¶å‘çŠ¶æ€åˆ—è¡¨
   */
  async getAllConcurrencyStatus() {
    try {
      const client = this.getClientSafe()
      const keys = await this.scanKeys('concurrency:*')
      const now = Date.now()
      const results = []

      for (const key of keys) {
        // è·³è¿‡å·²çŸ¥é Sorted Set ç±»å‹çš„é”®
        // - concurrency:queue:stats:* æ˜¯ Hash ç±»å‹
        // - concurrency:queue:wait_times:* æ˜¯ List ç±»å‹
        // - concurrency:queue:* (ä¸å«stats/wait_times) æ˜¯ String ç±»å‹
        if (
          key.startsWith('concurrency:queue:stats:') ||
          key.startsWith('concurrency:queue:wait_times:') ||
          (key.startsWith('concurrency:queue:') &&
            !key.includes(':stats:') &&
            !key.includes(':wait_times:'))
        ) {
          continue
        }

        // æ£€æŸ¥é”®ç±»å‹ï¼Œåªå¤„ç† Sorted Set
        const keyType = await client.type(key)
        if (keyType !== 'zset') {
          logger.debug(`ğŸ”¢ getAllConcurrencyStatus skipped non-zset key: ${key} (type: ${keyType})`)
          continue
        }

        // æå– apiKeyIdï¼ˆå»æ‰ concurrency: å‰ç¼€ï¼‰
        const apiKeyId = key.replace('concurrency:', '')

        // è·å–æ‰€æœ‰æˆå‘˜å’Œåˆ†æ•°ï¼ˆè¿‡æœŸæ—¶é—´ï¼‰
        const members = await client.zrangebyscore(key, now, '+inf', 'WITHSCORES')

        // è§£ææˆå‘˜å’Œè¿‡æœŸæ—¶é—´
        const activeRequests = []
        for (let i = 0; i < members.length; i += 2) {
          const requestId = members[i]
          const expireAt = parseInt(members[i + 1])
          const remainingSeconds = Math.max(0, Math.round((expireAt - now) / 1000))
          activeRequests.push({
            requestId,
            expireAt: new Date(expireAt).toISOString(),
            remainingSeconds
          })
        }

        // è·å–è¿‡æœŸçš„æˆå‘˜æ•°é‡
        const expiredCount = await client.zcount(key, '-inf', now)

        results.push({
          apiKeyId,
          key,
          activeCount: activeRequests.length,
          expiredCount,
          activeRequests
        })
      }

      return results
    } catch (error) {
      logger.error('âŒ Failed to get all concurrency status:', error)
      throw error
    }
  }

  /**
   * è·å–ç‰¹å®š API Key çš„å¹¶å‘çŠ¶æ€è¯¦æƒ…
   * @param {string} apiKeyId - API Key ID
   * @returns {Promise<Object>} å¹¶å‘çŠ¶æ€è¯¦æƒ…
   */
  async getConcurrencyStatus(apiKeyId) {
    try {
      const client = this.getClientSafe()
      const key = `concurrency:${apiKeyId}`
      const now = Date.now()

      // æ£€æŸ¥ key æ˜¯å¦å­˜åœ¨
      const exists = await client.exists(key)
      if (!exists) {
        return {
          apiKeyId,
          key,
          activeCount: 0,
          expiredCount: 0,
          activeRequests: [],
          exists: false
        }
      }

      // æ£€æŸ¥é”®ç±»å‹ï¼Œåªå¤„ç† Sorted Set
      const keyType = await client.type(key)
      if (keyType !== 'zset') {
        logger.warn(
          `âš ï¸ getConcurrencyStatus: key ${key} has unexpected type: ${keyType}, expected zset`
        )
        return {
          apiKeyId,
          key,
          activeCount: 0,
          expiredCount: 0,
          activeRequests: [],
          exists: true,
          invalidType: keyType
        }
      }

      // è·å–æ‰€æœ‰æˆå‘˜å’Œåˆ†æ•°
      const allMembers = await client.zrange(key, 0, -1, 'WITHSCORES')

      const activeRequests = []
      const expiredRequests = []

      for (let i = 0; i < allMembers.length; i += 2) {
        const requestId = allMembers[i]
        const expireAt = parseInt(allMembers[i + 1])
        const remainingSeconds = Math.round((expireAt - now) / 1000)

        const requestInfo = {
          requestId,
          expireAt: new Date(expireAt).toISOString(),
          remainingSeconds
        }

        if (expireAt > now) {
          activeRequests.push(requestInfo)
        } else {
          expiredRequests.push(requestInfo)
        }
      }

      return {
        apiKeyId,
        key,
        activeCount: activeRequests.length,
        expiredCount: expiredRequests.length,
        activeRequests,
        expiredRequests,
        exists: true
      }
    } catch (error) {
      logger.error(`âŒ Failed to get concurrency status for ${apiKeyId}:`, error)
      throw error
    }
  }

  /**
   * å¼ºåˆ¶æ¸…ç†ç‰¹å®š API Key çš„å¹¶å‘è®¡æ•°ï¼ˆå¿½ç•¥ç§Ÿçº¦ï¼‰
   * @param {string} apiKeyId - API Key ID
   * @returns {Promise<Object>} æ¸…ç†ç»“æœ
   */
  async forceClearConcurrency(apiKeyId) {
    try {
      const client = this.getClientSafe()
      const key = `concurrency:${apiKeyId}`

      // æ£€æŸ¥é”®ç±»å‹
      const keyType = await client.type(key)

      let beforeCount = 0
      let isLegacy = false

      if (keyType === 'zset') {
        // æ­£å¸¸çš„ zset é”®ï¼Œè·å–æ¡ç›®æ•°
        beforeCount = await client.zcard(key)
      } else if (keyType !== 'none') {
        // é zset ä¸”éç©ºçš„é—ç•™é”®
        isLegacy = true
        logger.warn(
          `âš ï¸ forceClearConcurrency: key ${key} has unexpected type: ${keyType}, will be deleted`
        )
      }

      // åˆ é™¤é”®ï¼ˆæ— è®ºä»€ä¹ˆç±»å‹ï¼‰
      await client.del(key)

      logger.warn(
        `ğŸ§¹ Force cleared concurrency for key ${apiKeyId}, removed ${beforeCount} entries${isLegacy ? ' (legacy key)' : ''}`
      )

      return {
        apiKeyId,
        key,
        clearedCount: beforeCount,
        type: keyType,
        legacy: isLegacy,
        success: true
      }
    } catch (error) {
      logger.error(`âŒ Failed to force clear concurrency for ${apiKeyId}:`, error)
      throw error
    }
  }

  /**
   * å¼ºåˆ¶æ¸…ç†æ‰€æœ‰å¹¶å‘è®¡æ•°
   * @returns {Promise<Object>} æ¸…ç†ç»“æœ
   */
  async forceClearAllConcurrency() {
    try {
      const client = this.getClientSafe()
      const keys = await this.scanKeys('concurrency:*')

      let totalCleared = 0
      let legacyCleared = 0
      const clearedKeys = []

      for (const key of keys) {
        // è·³è¿‡ queue ç›¸å…³çš„é”®ï¼ˆå®ƒä»¬æœ‰å„è‡ªçš„æ¸…ç†é€»è¾‘ï¼‰
        if (key.startsWith('concurrency:queue:')) {
          continue
        }

        // æ£€æŸ¥é”®ç±»å‹
        const keyType = await client.type(key)
        if (keyType === 'zset') {
          const count = await client.zcard(key)
          await client.del(key)
          totalCleared += count
          clearedKeys.push({
            key,
            clearedCount: count,
            type: 'zset'
          })
        } else {
          // é zset ç±»å‹çš„é—ç•™é”®ï¼Œç›´æ¥åˆ é™¤
          await client.del(key)
          legacyCleared++
          clearedKeys.push({
            key,
            clearedCount: 0,
            type: keyType,
            legacy: true
          })
        }
      }

      logger.warn(
        `ğŸ§¹ Force cleared all concurrency: ${clearedKeys.length} keys, ${totalCleared} entries, ${legacyCleared} legacy keys`
      )

      return {
        keysCleared: clearedKeys.length,
        totalEntriesCleared: totalCleared,
        legacyKeysCleared: legacyCleared,
        clearedKeys,
        success: true
      }
    } catch (error) {
      logger.error('âŒ Failed to force clear all concurrency:', error)
      throw error
    }
  }

  /**
   * æ¸…ç†è¿‡æœŸçš„å¹¶å‘æ¡ç›®ï¼ˆä¸å½±å“æ´»è·ƒè¯·æ±‚ï¼‰
   * @param {string} apiKeyId - API Key IDï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™æ¸…ç†æ‰€æœ‰ï¼‰
   * @returns {Promise<Object>} æ¸…ç†ç»“æœ
   */
  async cleanupExpiredConcurrency(apiKeyId = null) {
    try {
      const client = this.getClientSafe()
      const now = Date.now()
      let keys

      if (apiKeyId) {
        keys = [`concurrency:${apiKeyId}`]
      } else {
        keys = await this.scanKeys('concurrency:*')
      }

      let totalCleaned = 0
      let legacyCleaned = 0
      const cleanedKeys = []

      for (const key of keys) {
        // è·³è¿‡ queue ç›¸å…³çš„é”®ï¼ˆå®ƒä»¬æœ‰å„è‡ªçš„æ¸…ç†é€»è¾‘ï¼‰
        if (key.startsWith('concurrency:queue:')) {
          continue
        }

        // æ£€æŸ¥é”®ç±»å‹
        const keyType = await client.type(key)
        if (keyType !== 'zset') {
          // é zset ç±»å‹çš„é—ç•™é”®ï¼Œç›´æ¥åˆ é™¤
          await client.del(key)
          legacyCleaned++
          cleanedKeys.push({
            key,
            cleanedCount: 0,
            type: keyType,
            legacy: true
          })
          continue
        }

        // åªæ¸…ç†è¿‡æœŸçš„æ¡ç›®
        const cleaned = await client.zremrangebyscore(key, '-inf', now)
        if (cleaned > 0) {
          totalCleaned += cleaned
          cleanedKeys.push({
            key,
            cleanedCount: cleaned
          })
        }

        // å¦‚æœ key ä¸ºç©ºï¼Œåˆ é™¤å®ƒ
        const remaining = await client.zcard(key)
        if (remaining === 0) {
          await client.del(key)
        }
      }

      logger.info(
        `ğŸ§¹ Cleaned up expired concurrency: ${totalCleaned} entries from ${cleanedKeys.length} keys, ${legacyCleaned} legacy keys removed`
      )

      return {
        keysProcessed: keys.length,
        keysCleaned: cleanedKeys.length,
        totalEntriesCleaned: totalCleaned,
        legacyKeysRemoved: legacyCleaned,
        cleanedKeys,
        success: true
      }
    } catch (error) {
      logger.error('âŒ Failed to cleanup expired concurrency:', error)
      throw error
    }
  }

  // ğŸ”§ Basic Redis operations wrapper methods for convenience
  async get(key) {
    const client = this.getClientSafe()
    return await client.get(key)
  }

  async set(key, value, ...args) {
    const client = this.getClientSafe()
    return await client.set(key, value, ...args)
  }

  async setex(key, ttl, value) {
    const client = this.getClientSafe()
    return await client.setex(key, ttl, value)
  }

  async del(...keys) {
    const client = this.getClientSafe()
    return await client.del(...keys)
  }

  async keys(pattern) {
    return await this.scanKeys(pattern)
  }

  // ğŸ“Š æ‰¹é‡è·å–å¤šä¸ªè´¦æˆ·ä¼šè¯çª—å£å†…çš„ä½¿ç”¨ç»Ÿè®¡ï¼ˆåŒ…å«æ¨¡å‹ç»†åˆ†ï¼‰
  // è¯´æ˜ï¼šç®¡ç†åå°åˆ—è¡¨é¡µå¦‚æœé€è´¦æˆ·æŸ¥è¯¢ï¼Œä¼šè¢«è·¨æœº Redis RTT æ”¾å¤§ï¼›è¿™é‡Œç”¨å•/å°‘é‡ pipeline åˆå¹¶è¯»å–ã€‚
  async getAccountsSessionWindowUsage(accountWindows = [], options = {}) {
    const rawChunkSize = Number(options.chunkSize)
    const chunkSize = Number.isFinite(rawChunkSize) && rawChunkSize > 0 ? rawChunkSize : 1500

    const uniqueWindows = []
    const seenAccountIds = new Set()

    for (const item of Array.isArray(accountWindows) ? accountWindows : []) {
      const accountId = item?.accountId || item?.id
      if (!accountId || seenAccountIds.has(accountId)) {
        continue
      }
      if (!item?.windowStart || !item?.windowEnd) {
        continue
      }
      seenAccountIds.add(accountId)
      uniqueWindows.push({
        accountId,
        windowStart: item.windowStart,
        windowEnd: item.windowEnd
      })
    }

    const buildEmptyUsage = () => ({
      totalInputTokens: 0,
      totalOutputTokens: 0,
      totalCacheCreateTokens: 0,
      totalCacheReadTokens: 0,
      totalAllTokens: 0,
      totalRequests: 0,
      modelUsage: {}
    })

    const resultMap = Object.fromEntries(uniqueWindows.map((w) => [w.accountId, buildEmptyUsage()]))

    if (uniqueWindows.length === 0) {
      return resultMap
    }

    try {
      const keyMetas = []

      for (const window of uniqueWindows) {
        const startDate = new Date(window.windowStart)
        const endDate = new Date(window.windowEnd)

        if (Number.isNaN(startDate.getTime()) || Number.isNaN(endDate.getTime())) {
          continue
        }

        const currentHour = new Date(startDate)
        currentHour.setMinutes(0)
        currentHour.setSeconds(0)
        currentHour.setMilliseconds(0)

        while (currentHour <= endDate) {
          const tzDateStr = getDateStringInTimezone(currentHour)
          const tzHour = String(getHourInTimezone(currentHour)).padStart(2, '0')
          keyMetas.push({
            accountId: window.accountId,
            key: `account_usage:hourly:${window.accountId}:${tzDateStr}:${tzHour}`
          })
          currentHour.setHours(currentHour.getHours() + 1)
        }
      }

      for (let offset = 0; offset < keyMetas.length; offset += chunkSize) {
        const chunkMetas = keyMetas.slice(offset, offset + chunkSize)
        const pipeline = this.client.pipeline()
        for (const meta of chunkMetas) {
          pipeline.hgetall(meta.key)
        }
        const results = await pipeline.exec()

        for (let i = 0; i < chunkMetas.length; i++) {
          const meta = chunkMetas[i]
          const data = results?.[i]?.[1]
          if (!data || Object.keys(data).length === 0) {
            continue
          }

          if (!resultMap[meta.accountId]) {
            resultMap[meta.accountId] = buildEmptyUsage()
          }

          const aggregate = resultMap[meta.accountId]

          aggregate.totalInputTokens += parseInt(data.inputTokens || 0)
          aggregate.totalOutputTokens += parseInt(data.outputTokens || 0)
          aggregate.totalCacheCreateTokens += parseInt(data.cacheCreateTokens || 0)
          aggregate.totalCacheReadTokens += parseInt(data.cacheReadTokens || 0)
          aggregate.totalAllTokens += parseInt(data.allTokens || 0)
          aggregate.totalRequests += parseInt(data.requests || 0)

          for (const [key, value] of Object.entries(data)) {
            if (!key.startsWith('model:')) {
              continue
            }

            const parts = key.split(':')
            if (parts.length < 3) {
              continue
            }

            const modelName = parts[1]
            const metric = parts.slice(2).join(':')

            if (!aggregate.modelUsage[modelName]) {
              aggregate.modelUsage[modelName] = {
                inputTokens: 0,
                outputTokens: 0,
                cacheCreateTokens: 0,
                cacheReadTokens: 0,
                allTokens: 0,
                requests: 0
              }
            }

            const numeric = parseInt(value || 0)
            if (metric === 'inputTokens') {
              aggregate.modelUsage[modelName].inputTokens += numeric
            } else if (metric === 'outputTokens') {
              aggregate.modelUsage[modelName].outputTokens += numeric
            } else if (metric === 'cacheCreateTokens') {
              aggregate.modelUsage[modelName].cacheCreateTokens += numeric
            } else if (metric === 'cacheReadTokens') {
              aggregate.modelUsage[modelName].cacheReadTokens += numeric
            } else if (metric === 'allTokens') {
              aggregate.modelUsage[modelName].allTokens += numeric
            } else if (metric === 'requests') {
              aggregate.modelUsage[modelName].requests += numeric
            }
          }
        }
      }

      return resultMap
    } catch (error) {
      logger.error('âŒ Failed to batch get session window usage:', error)
      return resultMap
    }
  }

  // ğŸ“Š è·å–è´¦æˆ·ä¼šè¯çª—å£å†…çš„ä½¿ç”¨ç»Ÿè®¡ï¼ˆåŒ…å«æ¨¡å‹ç»†åˆ†ï¼‰
  async getAccountSessionWindowUsage(accountId, windowStart, windowEnd) {
    try {
      if (!windowStart || !windowEnd) {
        return {
          totalInputTokens: 0,
          totalOutputTokens: 0,
          totalCacheCreateTokens: 0,
          totalCacheReadTokens: 0,
          totalAllTokens: 0,
          totalRequests: 0,
          modelUsage: {}
        }
      }

      const startDate = new Date(windowStart)
      const endDate = new Date(windowEnd)
      const debugEnabled = ['debug', 'silly'].includes(logger.level)

      // æ·»åŠ æ—¥å¿—ä»¥è°ƒè¯•æ—¶é—´çª—å£
      if (debugEnabled) {
        logger.debug(`ğŸ“Š Getting session window usage for account ${accountId}`)
        logger.debug(`   Window: ${windowStart} to ${windowEnd}`)
        logger.debug(`   Start UTC: ${startDate.toISOString()}, End UTC: ${endDate.toISOString()}`)
      }

      // è·å–çª—å£å†…æ‰€æœ‰å¯èƒ½çš„å°æ—¶é”®
      // é‡è¦ï¼šéœ€è¦ä½¿ç”¨é…ç½®çš„æ—¶åŒºæ¥æ„å»ºé”®åï¼Œå› ä¸ºæ•°æ®å­˜å‚¨æ—¶ä½¿ç”¨çš„æ˜¯é…ç½®æ—¶åŒº
      const hourlyKeys = []
      const currentHour = new Date(startDate)
      currentHour.setMinutes(0)
      currentHour.setSeconds(0)
      currentHour.setMilliseconds(0)

      while (currentHour <= endDate) {
        // ä½¿ç”¨æ—¶åŒºè½¬æ¢å‡½æ•°æ¥è·å–æ­£ç¡®çš„æ—¥æœŸå’Œå°æ—¶
        const tzDateStr = getDateStringInTimezone(currentHour)
        const tzHour = String(getHourInTimezone(currentHour)).padStart(2, '0')
        const key = `account_usage:hourly:${accountId}:${tzDateStr}:${tzHour}`

        if (debugEnabled) {
          logger.debug(`   Adding hourly key: ${key}`)
        }
        hourlyKeys.push(key)
        currentHour.setHours(currentHour.getHours() + 1)
      }

      // æ‰¹é‡è·å–æ‰€æœ‰å°æ—¶çš„æ•°æ®
      const pipeline = this.client.pipeline()
      for (const key of hourlyKeys) {
        pipeline.hgetall(key)
      }
      const results = await pipeline.exec()

      // èšåˆæ‰€æœ‰æ•°æ®
      let totalInputTokens = 0
      let totalOutputTokens = 0
      let totalCacheCreateTokens = 0
      let totalCacheReadTokens = 0
      let totalAllTokens = 0
      let totalRequests = 0
      const modelUsage = {}

      if (debugEnabled) {
        logger.debug(`   Processing ${results.length} hourly results`)
      }

      for (const [error, data] of results) {
        if (error || !data || Object.keys(data).length === 0) {
          continue
        }

        // å¤„ç†æ€»è®¡æ•°æ®
        const hourInputTokens = parseInt(data.inputTokens || 0)
        const hourOutputTokens = parseInt(data.outputTokens || 0)
        const hourCacheCreateTokens = parseInt(data.cacheCreateTokens || 0)
        const hourCacheReadTokens = parseInt(data.cacheReadTokens || 0)
        const hourAllTokens = parseInt(data.allTokens || 0)
        const hourRequests = parseInt(data.requests || 0)

        totalInputTokens += hourInputTokens
        totalOutputTokens += hourOutputTokens
        totalCacheCreateTokens += hourCacheCreateTokens
        totalCacheReadTokens += hourCacheReadTokens
        totalAllTokens += hourAllTokens
        totalRequests += hourRequests

        if (debugEnabled && hourAllTokens > 0) {
          logger.debug(`   Hour data: allTokens=${hourAllTokens}, requests=${hourRequests}`)
        }

        // å¤„ç†æ¯ä¸ªæ¨¡å‹çš„æ•°æ®
        for (const [key, value] of Object.entries(data)) {
          // æŸ¥æ‰¾æ¨¡å‹ç›¸å…³çš„é”®ï¼ˆæ ¼å¼: model:{modelName}:{metric}ï¼‰
          if (key.startsWith('model:')) {
            const parts = key.split(':')
            if (parts.length >= 3) {
              const modelName = parts[1]
              const metric = parts.slice(2).join(':')

              if (!modelUsage[modelName]) {
                modelUsage[modelName] = {
                  inputTokens: 0,
                  outputTokens: 0,
                  cacheCreateTokens: 0,
                  cacheReadTokens: 0,
                  allTokens: 0,
                  requests: 0
                }
              }

              if (metric === 'inputTokens') {
                modelUsage[modelName].inputTokens += parseInt(value || 0)
              } else if (metric === 'outputTokens') {
                modelUsage[modelName].outputTokens += parseInt(value || 0)
              } else if (metric === 'cacheCreateTokens') {
                modelUsage[modelName].cacheCreateTokens += parseInt(value || 0)
              } else if (metric === 'cacheReadTokens') {
                modelUsage[modelName].cacheReadTokens += parseInt(value || 0)
              } else if (metric === 'allTokens') {
                modelUsage[modelName].allTokens += parseInt(value || 0)
              } else if (metric === 'requests') {
                modelUsage[modelName].requests += parseInt(value || 0)
              }
            }
          }
        }
      }

      if (debugEnabled) {
        logger.debug(`ğŸ“Š Session window usage summary:`)
        logger.debug(`   Total allTokens: ${totalAllTokens}`)
        logger.debug(`   Total requests: ${totalRequests}`)
        logger.debug(`   Input: ${totalInputTokens}, Output: ${totalOutputTokens}`)
        logger.debug(
          `   Cache Create: ${totalCacheCreateTokens}, Cache Read: ${totalCacheReadTokens}`
        )
      }

      return {
        totalInputTokens,
        totalOutputTokens,
        totalCacheCreateTokens,
        totalCacheReadTokens,
        totalAllTokens,
        totalRequests,
        modelUsage
      }
    } catch (error) {
      logger.error(`âŒ Failed to get session window usage for account ${accountId}:`, error)
      return {
        totalInputTokens: 0,
        totalOutputTokens: 0,
        totalCacheCreateTokens: 0,
        totalCacheReadTokens: 0,
        totalAllTokens: 0,
        totalRequests: 0,
        modelUsage: {}
      }
    }
  }
}

const redisClient = new RedisClient()

// åˆ†å¸ƒå¼é”ç›¸å…³æ–¹æ³•
redisClient.setAccountLock = async function (lockKey, lockValue, ttlMs) {
  // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
  if (await goRedisProxy.isAvailable()) {
    try {
      const ttlSeconds = Math.ceil(ttlMs / 1000)
      return await goRedisProxy.setAccountLock(lockKey, lockValue, ttlSeconds)
    } catch (error) {
      logger.warn(`âš ï¸ Go service setAccountLock failed, falling back to Redis: ${error.message}`)
    }
  }

  try {
    // ä½¿ç”¨SET NX PXå®ç°åŸå­æ€§çš„é”è·å–
    // ioredisè¯­æ³•: set(key, value, 'PX', milliseconds, 'NX')
    const result = await this.client.set(lockKey, lockValue, 'PX', ttlMs, 'NX')
    return result === 'OK'
  } catch (error) {
    logger.error(`Failed to acquire lock ${lockKey}:`, error)
    return false
  }
}

redisClient.releaseAccountLock = async function (lockKey, lockValue) {
  // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
  if (await goRedisProxy.isAvailable()) {
    try {
      return await goRedisProxy.releaseAccountLock(lockKey, lockValue)
    } catch (error) {
      logger.warn(`âš ï¸ Go service releaseAccountLock failed, falling back to Redis: ${error.message}`)
    }
  }

  try {
    // ä½¿ç”¨Luaè„šæœ¬ç¡®ä¿åªæœ‰æŒæœ‰é”çš„è¿›ç¨‹æ‰èƒ½é‡Šæ”¾é”
    const script = `
      if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
      else
        return 0
      end
    `
    // ioredisè¯­æ³•: eval(script, numberOfKeys, key1, key2, ..., arg1, arg2, ...)
    const result = await this.client.eval(script, 1, lockKey, lockValue)
    return result === 1
  } catch (error) {
    logger.error(`Failed to release lock ${lockKey}:`, error)
    return false
  }
}

// å¯¼å‡ºæ—¶åŒºè¾…åŠ©å‡½æ•°
redisClient.getDateInTimezone = getDateInTimezone
redisClient.getDateStringInTimezone = getDateStringInTimezone
redisClient.getHourInTimezone = getHourInTimezone
redisClient.getWeekStringInTimezone = getWeekStringInTimezone

// ============== ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—ç›¸å…³æ–¹æ³• ==============

/**
 * å°è¯•è·å–ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—é”
 * ä½¿ç”¨ Lua è„šæœ¬ä¿è¯åŸå­æ€§
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} requestId - è¯·æ±‚ID
 * @param {number} lockTtlMs - é” TTLï¼ˆæ¯«ç§’ï¼‰
 * @param {number} delayMs - è¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼‰
 * @returns {Promise<{acquired: boolean, waitMs: number}>}
 *   - acquired: æ˜¯å¦æˆåŠŸè·å–é”
 *   - waitMs: éœ€è¦ç­‰å¾…çš„æ¯«ç§’æ•°ï¼ˆ-1è¡¨ç¤ºè¢«å ç”¨éœ€ç­‰å¾…ï¼Œ>=0è¡¨ç¤ºéœ€è¦å»¶è¿Ÿçš„æ¯«ç§’æ•°ï¼‰
 */
redisClient.acquireUserMessageLock = async function (accountId, requestId, lockTtlMs, delayMs) {
  // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
  if (await goRedisProxy.isAvailable()) {
    try {
      return await goRedisProxy.acquireUserMessageLock(accountId, requestId, lockTtlMs, delayMs)
    } catch (error) {
      logger.warn(`âš ï¸ Go service acquireUserMessageLock failed, falling back to Redis: ${error.message}`)
    }
  }

  const lockKey = `user_msg_queue_lock:${accountId}`
  const lastTimeKey = `user_msg_queue_last:${accountId}`

  const script = `
    local lockKey = KEYS[1]
    local lastTimeKey = KEYS[2]
    local requestId = ARGV[1]
    local lockTtl = tonumber(ARGV[2])
    local delayMs = tonumber(ARGV[3])

    -- æ£€æŸ¥é”æ˜¯å¦ç©ºé—²
    local currentLock = redis.call('GET', lockKey)
    if currentLock == false then
      -- æ£€æŸ¥æ˜¯å¦éœ€è¦å»¶è¿Ÿ
      local lastTime = redis.call('GET', lastTimeKey)
      local now = redis.call('TIME')
      local nowMs = tonumber(now[1]) * 1000 + math.floor(tonumber(now[2]) / 1000)

      if lastTime then
        local elapsed = nowMs - tonumber(lastTime)
        if elapsed < delayMs then
          -- éœ€è¦ç­‰å¾…çš„æ¯«ç§’æ•°
          return {0, delayMs - elapsed}
        end
      end

      -- è·å–é”
      redis.call('SET', lockKey, requestId, 'PX', lockTtl)
      return {1, 0}
    end

    -- é”è¢«å ç”¨ï¼Œè¿”å›ç­‰å¾…
    return {0, -1}
  `

  try {
    const result = await this.client.eval(
      script,
      2,
      lockKey,
      lastTimeKey,
      requestId,
      lockTtlMs,
      delayMs
    )
    return {
      acquired: result[0] === 1,
      waitMs: result[1]
    }
  } catch (error) {
    logger.error(`Failed to acquire user message lock for account ${accountId}:`, error)
    // è¿”å› redisError æ ‡è®°ï¼Œè®©ä¸Šå±‚èƒ½åŒºåˆ† Redis æ•…éšœå’Œæ­£å¸¸é”å ç”¨
    return { acquired: false, waitMs: -1, redisError: true, errorMessage: error.message }
  }
}

/**
 * é‡Šæ”¾ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—é”å¹¶è®°å½•å®Œæˆæ—¶é—´
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} requestId - è¯·æ±‚ID
 * @returns {Promise<boolean>} æ˜¯å¦æˆåŠŸé‡Šæ”¾
 */
redisClient.releaseUserMessageLock = async function (accountId, requestId) {
  // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
  if (await goRedisProxy.isAvailable()) {
    try {
      return await goRedisProxy.releaseUserMessageLock(accountId, requestId)
    } catch (error) {
      logger.warn(`âš ï¸ Go service releaseUserMessageLock failed, falling back to Redis: ${error.message}`)
    }
  }

  const lockKey = `user_msg_queue_lock:${accountId}`
  const lastTimeKey = `user_msg_queue_last:${accountId}`

  const script = `
    local lockKey = KEYS[1]
    local lastTimeKey = KEYS[2]
    local requestId = ARGV[1]

    -- éªŒè¯é”æŒæœ‰è€…
    local currentLock = redis.call('GET', lockKey)
    if currentLock == requestId then
      -- è®°å½•å®Œæˆæ—¶é—´
      local now = redis.call('TIME')
      local nowMs = tonumber(now[1]) * 1000 + math.floor(tonumber(now[2]) / 1000)
      redis.call('SET', lastTimeKey, nowMs, 'EX', 60)  -- 60ç§’åè¿‡æœŸ

      -- åˆ é™¤é”
      redis.call('DEL', lockKey)
      return 1
    end
    return 0
  `

  try {
    const result = await this.client.eval(script, 2, lockKey, lastTimeKey, requestId)
    return result === 1
  } catch (error) {
    logger.error(`Failed to release user message lock for account ${accountId}:`, error)
    return false
  }
}

/**
 * å¼ºåˆ¶é‡Šæ”¾ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—é”ï¼ˆç”¨äºæ¸…ç†å­¤å„¿é”ï¼‰
 * @param {string} accountId - è´¦æˆ·ID
 * @returns {Promise<boolean>} æ˜¯å¦æˆåŠŸé‡Šæ”¾
 */
redisClient.forceReleaseUserMessageLock = async function (accountId) {
  // ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡
  if (await goRedisProxy.isAvailable()) {
    try {
      return await goRedisProxy.forceReleaseUserMessageLock(accountId)
    } catch (error) {
      logger.warn(`âš ï¸ Go service forceReleaseUserMessageLock failed, falling back to Redis: ${error.message}`)
    }
  }

  const lockKey = `user_msg_queue_lock:${accountId}`

  try {
    await this.client.del(lockKey)
    return true
  } catch (error) {
    logger.error(`Failed to force release user message lock for account ${accountId}:`, error)
    return false
  }
}

/**
 * è·å–ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
 * @param {string} accountId - è´¦æˆ·ID
 * @returns {Promise<Object>} é˜Ÿåˆ—ç»Ÿè®¡
 */
redisClient.getUserMessageQueueStats = async function (accountId) {
  const lockKey = `user_msg_queue_lock:${accountId}`
  const lastTimeKey = `user_msg_queue_last:${accountId}`

  try {
    const [lockHolder, lastTime, lockTtl] = await Promise.all([
      this.client.get(lockKey),
      this.client.get(lastTimeKey),
      this.client.pttl(lockKey)
    ])

    return {
      accountId,
      isLocked: !!lockHolder,
      lockHolder,
      lockTtlMs: lockTtl > 0 ? lockTtl : 0,
      lockTtlRaw: lockTtl, // åŸå§‹ PTTL å€¼ï¼š>0 æœ‰TTLï¼Œ-1 æ— è¿‡æœŸæ—¶é—´ï¼Œ-2 é”®ä¸å­˜åœ¨
      lastCompletedAt: lastTime ? new Date(parseInt(lastTime)).toISOString() : null
    }
  } catch (error) {
    logger.error(`Failed to get user message queue stats for account ${accountId}:`, error)
    return {
      accountId,
      isLocked: false,
      lockHolder: null,
      lockTtlMs: 0,
      lockTtlRaw: -2,
      lastCompletedAt: null
    }
  }
}

/**
 * æ‰«ææ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—é”ï¼ˆç”¨äºæ¸…ç†ä»»åŠ¡ï¼‰
 * @returns {Promise<string[]>} è´¦æˆ·IDåˆ—è¡¨
 */
redisClient.scanUserMessageQueueLocks = async function () {
  const accountIds = []
  let cursor = '0'
  let iterations = 0
  const MAX_ITERATIONS = 1000 // é˜²æ­¢æ— é™å¾ªç¯

  try {
    do {
      const [newCursor, keys] = await this.client.scan(
        cursor,
        'MATCH',
        'user_msg_queue_lock:*',
        'COUNT',
        100
      )
      cursor = newCursor
      iterations++

      for (const key of keys) {
        const accountId = key.replace('user_msg_queue_lock:', '')
        accountIds.push(accountId)
      }

      // é˜²æ­¢æ— é™å¾ªç¯
      if (iterations >= MAX_ITERATIONS) {
        logger.warn(
          `ğŸ“¬ User message queue: SCAN reached max iterations (${MAX_ITERATIONS}), stopping early`,
          { foundLocks: accountIds.length }
        )
        break
      }
    } while (cursor !== '0')

    if (accountIds.length > 0) {
      logger.debug(
        `ğŸ“¬ User message queue: scanned ${accountIds.length} lock(s) in ${iterations} iteration(s)`
      )
    }

    return accountIds
  } catch (error) {
    logger.error('Failed to scan user message queue locks:', error)
    return []
  }
}

// ============================================
// ğŸš¦ API Key å¹¶å‘è¯·æ±‚æ’é˜Ÿæ–¹æ³•
// ============================================

/**
 * å¢åŠ æ’é˜Ÿè®¡æ•°ï¼ˆä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åŸå­æ€§ï¼‰
 * @param {string} apiKeyId - API Key ID
 * @param {number} [timeoutMs=60000] - æ’é˜Ÿè¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºè®¡ç®— TTL
 * @returns {Promise<number>} å¢åŠ åçš„æ’é˜Ÿæ•°é‡
 */
redisClient.incrConcurrencyQueue = async function (apiKeyId, timeoutMs = 60000) {
  const key = `concurrency:queue:${apiKeyId}`
  try {
    // ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿ INCR å’Œ EXPIRE åŸå­æ‰§è¡Œï¼Œé˜²æ­¢è¿›ç¨‹å´©æºƒå¯¼è‡´è®¡æ•°å™¨æ³„æ¼
    // TTL = è¶…æ—¶æ—¶é—´ + ç¼“å†²æ—¶é—´ï¼ˆç¡®ä¿é”®ä¸ä¼šåœ¨è¯·æ±‚è¿˜åœ¨ç­‰å¾…æ—¶è¿‡æœŸï¼‰
    const ttlSeconds = Math.ceil(timeoutMs / 1000) + QUEUE_TTL_BUFFER_SECONDS
    const script = `
      local count = redis.call('INCR', KEYS[1])
      redis.call('EXPIRE', KEYS[1], ARGV[1])
      return count
    `
    const count = await this.client.eval(script, 1, key, String(ttlSeconds))
    logger.database(
      `ğŸš¦ Incremented queue count for key ${apiKeyId}: ${count} (TTL: ${ttlSeconds}s)`
    )
    return parseInt(count)
  } catch (error) {
    logger.error(`Failed to increment concurrency queue for ${apiKeyId}:`, error)
    throw error
  }
}

/**
 * å‡å°‘æ’é˜Ÿè®¡æ•°ï¼ˆä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åŸå­æ€§ï¼‰
 * @param {string} apiKeyId - API Key ID
 * @returns {Promise<number>} å‡å°‘åçš„æ’é˜Ÿæ•°é‡
 */
redisClient.decrConcurrencyQueue = async function (apiKeyId) {
  const key = `concurrency:queue:${apiKeyId}`
  try {
    // ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿ DECR å’Œ DEL åŸå­æ‰§è¡Œï¼Œé˜²æ­¢è¿›ç¨‹å´©æºƒå¯¼è‡´è®¡æ•°å™¨æ®‹ç•™
    const script = `
      local count = redis.call('DECR', KEYS[1])
      if count <= 0 then
        redis.call('DEL', KEYS[1])
        return 0
      end
      return count
    `
    const count = await this.client.eval(script, 1, key)
    const result = parseInt(count)
    if (result === 0) {
      logger.database(`ğŸš¦ Queue count for key ${apiKeyId} is 0, removed key`)
    } else {
      logger.database(`ğŸš¦ Decremented queue count for key ${apiKeyId}: ${result}`)
    }
    return result
  } catch (error) {
    logger.error(`Failed to decrement concurrency queue for ${apiKeyId}:`, error)
    throw error
  }
}

/**
 * è·å–æ’é˜Ÿè®¡æ•°
 * @param {string} apiKeyId - API Key ID
 * @returns {Promise<number>} å½“å‰æ’é˜Ÿæ•°é‡
 */
redisClient.getConcurrencyQueueCount = async function (apiKeyId) {
  const key = `concurrency:queue:${apiKeyId}`
  try {
    const count = await this.client.get(key)
    return parseInt(count || 0)
  } catch (error) {
    logger.error(`Failed to get concurrency queue count for ${apiKeyId}:`, error)
    return 0
  }
}

/**
 * æ¸…ç©ºæ’é˜Ÿè®¡æ•°
 * @param {string} apiKeyId - API Key ID
 * @returns {Promise<boolean>} æ˜¯å¦æˆåŠŸæ¸…ç©º
 */
redisClient.clearConcurrencyQueue = async function (apiKeyId) {
  const key = `concurrency:queue:${apiKeyId}`
  try {
    await this.client.del(key)
    logger.database(`ğŸš¦ Cleared queue count for key ${apiKeyId}`)
    return true
  } catch (error) {
    logger.error(`Failed to clear concurrency queue for ${apiKeyId}:`, error)
    return false
  }
}

/**
 * æ‰«ææ‰€æœ‰æ’é˜Ÿè®¡æ•°å™¨
 * @returns {Promise<string[]>} API Key ID åˆ—è¡¨
 */
redisClient.scanConcurrencyQueueKeys = async function () {
  const apiKeyIds = []
  let cursor = '0'
  let iterations = 0
  const MAX_ITERATIONS = 1000

  try {
    do {
      const [newCursor, keys] = await this.client.scan(
        cursor,
        'MATCH',
        'concurrency:queue:*',
        'COUNT',
        100
      )
      cursor = newCursor
      iterations++

      for (const key of keys) {
        // æ’é™¤ç»Ÿè®¡å’Œç­‰å¾…æ—¶é—´ç›¸å…³çš„é”®
        if (
          key.startsWith('concurrency:queue:stats:') ||
          key.startsWith('concurrency:queue:wait_times:')
        ) {
          continue
        }
        const apiKeyId = key.replace('concurrency:queue:', '')
        apiKeyIds.push(apiKeyId)
      }

      if (iterations >= MAX_ITERATIONS) {
        logger.warn(
          `ğŸš¦ Concurrency queue: SCAN reached max iterations (${MAX_ITERATIONS}), stopping early`,
          { foundQueues: apiKeyIds.length }
        )
        break
      }
    } while (cursor !== '0')

    return apiKeyIds
  } catch (error) {
    logger.error('Failed to scan concurrency queue keys:', error)
    return []
  }
}

/**
 * æ¸…ç†æ‰€æœ‰æ’é˜Ÿè®¡æ•°å™¨ï¼ˆç”¨äºæœåŠ¡é‡å¯ï¼‰
 * @returns {Promise<number>} æ¸…ç†çš„è®¡æ•°å™¨æ•°é‡
 */
redisClient.clearAllConcurrencyQueues = async function () {
  let cleared = 0
  let cursor = '0'
  let iterations = 0
  const MAX_ITERATIONS = 1000

  try {
    do {
      const [newCursor, keys] = await this.client.scan(
        cursor,
        'MATCH',
        'concurrency:queue:*',
        'COUNT',
        100
      )
      cursor = newCursor
      iterations++

      // åªåˆ é™¤æ’é˜Ÿè®¡æ•°å™¨ï¼Œä¿ç•™ç»Ÿè®¡æ•°æ®
      const queueKeys = keys.filter(
        (key) =>
          !key.startsWith('concurrency:queue:stats:') &&
          !key.startsWith('concurrency:queue:wait_times:')
      )

      if (queueKeys.length > 0) {
        await this.client.del(...queueKeys)
        cleared += queueKeys.length
      }

      if (iterations >= MAX_ITERATIONS) {
        break
      }
    } while (cursor !== '0')

    if (cleared > 0) {
      logger.info(`ğŸš¦ Cleared ${cleared} concurrency queue counter(s) on startup`)
    }
    return cleared
  } catch (error) {
    logger.error('Failed to clear all concurrency queues:', error)
    return 0
  }
}

/**
 * å¢åŠ æ’é˜Ÿç»Ÿè®¡è®¡æ•°ï¼ˆä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åŸå­æ€§ï¼‰
 * @param {string} apiKeyId - API Key ID
 * @param {string} field - ç»Ÿè®¡å­—æ®µ (entered/success/timeout/cancelled)
 * @returns {Promise<number>} å¢åŠ åçš„è®¡æ•°
 */
redisClient.incrConcurrencyQueueStats = async function (apiKeyId, field) {
  const key = `concurrency:queue:stats:${apiKeyId}`
  try {
    // ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿ HINCRBY å’Œ EXPIRE åŸå­æ‰§è¡Œ
    // é˜²æ­¢åœ¨ä¸¤è€…ä¹‹é—´å´©æºƒå¯¼è‡´ç»Ÿè®¡é”®æ²¡æœ‰ TTLï¼ˆå†…å­˜æ³„æ¼ï¼‰
    const script = `
      local count = redis.call('HINCRBY', KEYS[1], ARGV[1], 1)
      redis.call('EXPIRE', KEYS[1], ARGV[2])
      return count
    `
    const count = await this.client.eval(script, 1, key, field, String(QUEUE_STATS_TTL_SECONDS))
    return parseInt(count)
  } catch (error) {
    logger.error(`Failed to increment queue stats ${field} for ${apiKeyId}:`, error)
    return 0
  }
}

/**
 * è·å–æ’é˜Ÿç»Ÿè®¡
 * @param {string} apiKeyId - API Key ID
 * @returns {Promise<Object>} ç»Ÿè®¡æ•°æ®
 */
redisClient.getConcurrencyQueueStats = async function (apiKeyId) {
  const key = `concurrency:queue:stats:${apiKeyId}`
  try {
    const stats = await this.client.hgetall(key)
    return {
      entered: parseInt(stats?.entered || 0),
      success: parseInt(stats?.success || 0),
      timeout: parseInt(stats?.timeout || 0),
      cancelled: parseInt(stats?.cancelled || 0),
      socket_changed: parseInt(stats?.socket_changed || 0),
      rejected_overload: parseInt(stats?.rejected_overload || 0)
    }
  } catch (error) {
    logger.error(`Failed to get queue stats for ${apiKeyId}:`, error)
    return {
      entered: 0,
      success: 0,
      timeout: 0,
      cancelled: 0,
      socket_changed: 0,
      rejected_overload: 0
    }
  }
}

/**
 * è®°å½•æ’é˜Ÿç­‰å¾…æ—¶é—´ï¼ˆæŒ‰ API Key åˆ†å¼€å­˜å‚¨ï¼‰
 * @param {string} apiKeyId - API Key ID
 * @param {number} waitTimeMs - ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
 * @returns {Promise<void>}
 */
redisClient.recordQueueWaitTime = async function (apiKeyId, waitTimeMs) {
  const key = `concurrency:queue:wait_times:${apiKeyId}`
  try {
    // ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åŸå­æ€§ï¼ŒåŒæ—¶è®¾ç½® TTL é˜²æ­¢å†…å­˜æ³„æ¼
    const script = `
      redis.call('LPUSH', KEYS[1], ARGV[1])
      redis.call('LTRIM', KEYS[1], 0, ARGV[2])
      redis.call('EXPIRE', KEYS[1], ARGV[3])
      return 1
    `
    await this.client.eval(
      script,
      1,
      key,
      waitTimeMs,
      WAIT_TIME_SAMPLES_PER_KEY - 1,
      WAIT_TIME_TTL_SECONDS
    )
  } catch (error) {
    logger.error(`Failed to record queue wait time for ${apiKeyId}:`, error)
  }
}

/**
 * è®°å½•å…¨å±€æ’é˜Ÿç­‰å¾…æ—¶é—´
 * @param {number} waitTimeMs - ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
 * @returns {Promise<void>}
 */
redisClient.recordGlobalQueueWaitTime = async function (waitTimeMs) {
  const key = 'concurrency:queue:wait_times:global'
  try {
    // ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åŸå­æ€§ï¼ŒåŒæ—¶è®¾ç½® TTL é˜²æ­¢å†…å­˜æ³„æ¼
    const script = `
      redis.call('LPUSH', KEYS[1], ARGV[1])
      redis.call('LTRIM', KEYS[1], 0, ARGV[2])
      redis.call('EXPIRE', KEYS[1], ARGV[3])
      return 1
    `
    await this.client.eval(
      script,
      1,
      key,
      waitTimeMs,
      WAIT_TIME_SAMPLES_GLOBAL - 1,
      WAIT_TIME_TTL_SECONDS
    )
  } catch (error) {
    logger.error('Failed to record global queue wait time:', error)
  }
}

/**
 * è·å–å…¨å±€ç­‰å¾…æ—¶é—´åˆ—è¡¨
 * @returns {Promise<number[]>} ç­‰å¾…æ—¶é—´åˆ—è¡¨
 */
redisClient.getGlobalQueueWaitTimes = async function () {
  const key = 'concurrency:queue:wait_times:global'
  try {
    const samples = await this.client.lrange(key, 0, -1)
    return samples.map(Number)
  } catch (error) {
    logger.error('Failed to get global queue wait times:', error)
    return []
  }
}

/**
 * è·å–æŒ‡å®š API Key çš„ç­‰å¾…æ—¶é—´åˆ—è¡¨
 * @param {string} apiKeyId - API Key ID
 * @returns {Promise<number[]>} ç­‰å¾…æ—¶é—´åˆ—è¡¨
 */
redisClient.getQueueWaitTimes = async function (apiKeyId) {
  const key = `concurrency:queue:wait_times:${apiKeyId}`
  try {
    const samples = await this.client.lrange(key, 0, -1)
    return samples.map(Number)
  } catch (error) {
    logger.error(`Failed to get queue wait times for ${apiKeyId}:`, error)
    return []
  }
}

/**
 * æ‰«ææ‰€æœ‰æ’é˜Ÿç»Ÿè®¡é”®
 * @returns {Promise<string[]>} API Key ID åˆ—è¡¨
 */
redisClient.scanConcurrencyQueueStatsKeys = async function () {
  const apiKeyIds = []
  let cursor = '0'
  let iterations = 0
  const MAX_ITERATIONS = 1000

  try {
    do {
      const [newCursor, keys] = await this.client.scan(
        cursor,
        'MATCH',
        'concurrency:queue:stats:*',
        'COUNT',
        100
      )
      cursor = newCursor
      iterations++

      for (const key of keys) {
        const apiKeyId = key.replace('concurrency:queue:stats:', '')
        apiKeyIds.push(apiKeyId)
      }

      if (iterations >= MAX_ITERATIONS) {
        break
      }
    } while (cursor !== '0')

    return apiKeyIds
  } catch (error) {
    logger.error('Failed to scan concurrency queue stats keys:', error)
    return []
  }
}

// ============================================================================
// è´¦æˆ·æµ‹è¯•å†å²ç›¸å…³æ“ä½œ
// ============================================================================

const ACCOUNT_TEST_HISTORY_MAX = 5 // ä¿ç•™æœ€è¿‘5æ¬¡æµ‹è¯•è®°å½•
const ACCOUNT_TEST_HISTORY_TTL = 86400 * 30 // 30å¤©è¿‡æœŸ
const ACCOUNT_TEST_CONFIG_TTL = 86400 * 365 // æµ‹è¯•é…ç½®ä¿ç•™1å¹´ï¼ˆç”¨æˆ·é€šå¸¸é•¿æœŸä½¿ç”¨ï¼‰

/**
 * ä¿å­˜è´¦æˆ·æµ‹è¯•ç»“æœ
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹ (claude/gemini/openaiç­‰)
 * @param {Object} testResult - æµ‹è¯•ç»“æœå¯¹è±¡
 * @param {boolean} testResult.success - æ˜¯å¦æˆåŠŸ
 * @param {string} testResult.message - æµ‹è¯•æ¶ˆæ¯/å“åº”
 * @param {number} testResult.latencyMs - å»¶è¿Ÿæ¯«ç§’æ•°
 * @param {string} testResult.error - é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
 * @param {string} testResult.timestamp - æµ‹è¯•æ—¶é—´æˆ³
 */
redisClient.saveAccountTestResult = async function (accountId, platform, testResult) {
  const key = `account:test_history:${platform}:${accountId}`
  try {
    const record = JSON.stringify({
      ...testResult,
      timestamp: testResult.timestamp || new Date().toISOString()
    })

    // ä½¿ç”¨ LPUSH + LTRIM ä¿æŒæœ€è¿‘5æ¡è®°å½•
    const client = this.getClientSafe()
    await client.lpush(key, record)
    await client.ltrim(key, 0, ACCOUNT_TEST_HISTORY_MAX - 1)
    await client.expire(key, ACCOUNT_TEST_HISTORY_TTL)

    logger.debug(`ğŸ“ Saved test result for ${platform} account ${accountId}`)
  } catch (error) {
    logger.error(`Failed to save test result for ${accountId}:`, error)
  }
}

/**
 * è·å–è´¦æˆ·æµ‹è¯•å†å²
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹
 * @returns {Promise<Array>} æµ‹è¯•å†å²è®°å½•æ•°ç»„ï¼ˆæœ€æ–°åœ¨å‰ï¼‰
 */
redisClient.getAccountTestHistory = async function (accountId, platform) {
  const key = `account:test_history:${platform}:${accountId}`
  try {
    const client = this.getClientSafe()
    const records = await client.lrange(key, 0, -1)
    return records.map((r) => JSON.parse(r))
  } catch (error) {
    logger.error(`Failed to get test history for ${accountId}:`, error)
    return []
  }
}

/**
 * è·å–è´¦æˆ·æœ€æ–°æµ‹è¯•ç»“æœ
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹
 * @returns {Promise<Object|null>} æœ€æ–°æµ‹è¯•ç»“æœ
 */
redisClient.getAccountLatestTestResult = async function (accountId, platform) {
  const key = `account:test_history:${platform}:${accountId}`
  try {
    const client = this.getClientSafe()
    const record = await client.lindex(key, 0)
    return record ? JSON.parse(record) : null
  } catch (error) {
    logger.error(`Failed to get latest test result for ${accountId}:`, error)
    return null
  }
}

/**
 * æ‰¹é‡è·å–å¤šä¸ªè´¦æˆ·çš„æµ‹è¯•å†å²
 * @param {Array<{accountId: string, platform: string}>} accounts - è´¦æˆ·åˆ—è¡¨
 * @returns {Promise<Object>} ä»¥ accountId ä¸º key çš„æµ‹è¯•å†å²æ˜ å°„
 */
redisClient.getAccountsTestHistory = async function (accounts) {
  const result = {}
  try {
    const client = this.getClientSafe()
    const pipeline = client.pipeline()

    for (const { accountId, platform } of accounts) {
      const key = `account:test_history:${platform}:${accountId}`
      pipeline.lrange(key, 0, -1)
    }

    const responses = await pipeline.exec()

    accounts.forEach(({ accountId }, index) => {
      const [err, records] = responses[index]
      if (!err && records) {
        result[accountId] = records.map((r) => JSON.parse(r))
      } else {
        result[accountId] = []
      }
    })
  } catch (error) {
    logger.error('Failed to get batch test history:', error)
  }
  return result
}

/**
 * ä¿å­˜å®šæ—¶æµ‹è¯•é…ç½®
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹
 * @param {Object} config - é…ç½®å¯¹è±¡
 * @param {boolean} config.enabled - æ˜¯å¦å¯ç”¨å®šæ—¶æµ‹è¯•
 * @param {string} config.cronExpression - Cron è¡¨è¾¾å¼ (å¦‚ "0 8 * * *" è¡¨ç¤ºæ¯å¤©8ç‚¹)
 * @param {string} config.model - æµ‹è¯•ä½¿ç”¨çš„æ¨¡å‹
 */
redisClient.saveAccountTestConfig = async function (accountId, platform, testConfig) {
  const key = `account:test_config:${platform}:${accountId}`
  try {
    const client = this.getClientSafe()
    await client.hset(key, {
      enabled: testConfig.enabled ? 'true' : 'false',
      cronExpression: testConfig.cronExpression || '0 8 * * *', // é»˜è®¤æ¯å¤©æ—©ä¸Š8ç‚¹
      model: testConfig.model || 'claude-sonnet-4-5-20250929', // é»˜è®¤æ¨¡å‹
      updatedAt: new Date().toISOString()
    })
    // è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ1å¹´ï¼‰
    await client.expire(key, ACCOUNT_TEST_CONFIG_TTL)
  } catch (error) {
    logger.error(`Failed to save test config for ${accountId}:`, error)
  }
}

/**
 * è·å–å®šæ—¶æµ‹è¯•é…ç½®
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹
 * @returns {Promise<Object|null>} é…ç½®å¯¹è±¡
 */
redisClient.getAccountTestConfig = async function (accountId, platform) {
  const key = `account:test_config:${platform}:${accountId}`
  try {
    const client = this.getClientSafe()
    const testConfig = await client.hgetall(key)
    if (!testConfig || Object.keys(testConfig).length === 0) {
      return null
    }
    // å‘åå…¼å®¹ï¼šå¦‚æœå­˜åœ¨æ—§çš„ testHour å­—æ®µï¼Œè½¬æ¢ä¸º cron è¡¨è¾¾å¼
    let { cronExpression } = testConfig
    if (!cronExpression && testConfig.testHour) {
      const hour = parseInt(testConfig.testHour, 10)
      cronExpression = `0 ${hour} * * *`
    }
    return {
      enabled: testConfig.enabled === 'true',
      cronExpression: cronExpression || '0 8 * * *',
      model: testConfig.model || 'claude-sonnet-4-5-20250929',
      updatedAt: testConfig.updatedAt
    }
  } catch (error) {
    logger.error(`Failed to get test config for ${accountId}:`, error)
    return null
  }
}

/**
 * è·å–æ‰€æœ‰å¯ç”¨å®šæ—¶æµ‹è¯•çš„è´¦æˆ·
 * @param {string} platform - å¹³å°ç±»å‹
 * @returns {Promise<Array>} è´¦æˆ·IDåˆ—è¡¨åŠ cron é…ç½®
 */
redisClient.getEnabledTestAccounts = async function (platform) {
  const accountIds = []
  let cursor = '0'

  try {
    const client = this.getClientSafe()
    do {
      const [newCursor, keys] = await client.scan(
        cursor,
        'MATCH',
        `account:test_config:${platform}:*`,
        'COUNT',
        100
      )
      cursor = newCursor

      for (const key of keys) {
        const testConfig = await client.hgetall(key)
        if (testConfig && testConfig.enabled === 'true') {
          const accountId = key.replace(`account:test_config:${platform}:`, '')
          // å‘åå…¼å®¹ï¼šå¦‚æœå­˜åœ¨æ—§çš„ testHour å­—æ®µï¼Œè½¬æ¢ä¸º cron è¡¨è¾¾å¼
          let { cronExpression } = testConfig
          if (!cronExpression && testConfig.testHour) {
            const hour = parseInt(testConfig.testHour, 10)
            cronExpression = `0 ${hour} * * *`
          }
          accountIds.push({
            accountId,
            cronExpression: cronExpression || '0 8 * * *',
            model: testConfig.model || 'claude-sonnet-4-5-20250929'
          })
        }
      }
    } while (cursor !== '0')

    return accountIds
  } catch (error) {
    logger.error(`Failed to get enabled test accounts for ${platform}:`, error)
    return []
  }
}

/**
 * ä¿å­˜è´¦æˆ·ä¸Šæ¬¡æµ‹è¯•æ—¶é—´ï¼ˆç”¨äºè°ƒåº¦å™¨åˆ¤æ–­æ˜¯å¦éœ€è¦æµ‹è¯•ï¼‰
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹
 */
redisClient.setAccountLastTestTime = async function (accountId, platform) {
  const key = `account:last_test:${platform}:${accountId}`
  try {
    const client = this.getClientSafe()
    await client.set(key, Date.now().toString(), 'EX', 86400 * 7) // 7å¤©è¿‡æœŸ
  } catch (error) {
    logger.error(`Failed to set last test time for ${accountId}:`, error)
  }
}

/**
 * è·å–è´¦æˆ·ä¸Šæ¬¡æµ‹è¯•æ—¶é—´
 * @param {string} accountId - è´¦æˆ·ID
 * @param {string} platform - å¹³å°ç±»å‹
 * @returns {Promise<number|null>} ä¸Šæ¬¡æµ‹è¯•æ—¶é—´æˆ³
 */
redisClient.getAccountLastTestTime = async function (accountId, platform) {
  const key = `account:last_test:${platform}:${accountId}`
  try {
    const client = this.getClientSafe()
    const timestamp = await client.get(key)
    return timestamp ? parseInt(timestamp, 10) : null
  } catch (error) {
    logger.error(`Failed to get last test time for ${accountId}:`, error)
    return null
  }
}

module.exports = redisClient
